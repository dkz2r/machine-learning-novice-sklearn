<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to Machine Learning with Scikit Learn: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><span class="badge text-bg-warning">
          <abbr title="This lesson is in the alpha phase, which means that it has been taught once and lesson authors are iterating on feedback.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-triangle" style="border-radius: 5px"></i>
              Alpha
            </a>
            <span class="visually-hidden">This lesson is in the alpha phase, which means that it has been taught once and lesson authors are iterating on feedback.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to Machine Learning with Scikit Learn
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to Machine Learning with Scikit Learn
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Machine Learning with Scikit Learn
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-regression.html">2. Supervised methods - Regression</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-classification.html">3. Supervised methods - Classification</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-ensemble-methods.html">4. Ensemble methods</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-clustering.html">5. Unsupervised methods - Clustering</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-dimensionality-reduction.html">6. Unsupervised methods - Dimensionality reduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-neural-networks.html">7. Neural Networks</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-ethics.html">8. Ethics and the Implications of Machine Learning</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="09-learn-more.html">9. Find out more</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction</a></p>
<hr>
<p>Last updated on 2025-07-23 |

        <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is machine learning?</li>
<li>What are some useful machine learning techniques?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Gain an overview of what machine learning is and the techniques
available.</li>
<li>Understand how machine learning and artificial intelligence
differ.</li>
<li>Be aware of some caveats when using Machine Learning.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="what-is-machine-learning">What is machine learning?<a class="anchor" aria-label="anchor" href="#what-is-machine-learning"></a>
</h2>
<hr class="half-width">
<p>Machine learning is a set of techniques that enable computers to use
data to improve their performance in a given task. This is similar in
concept to how humans learn to make predictions based upon previous
experience and knowledge. Machine learning encompasses a wide range of
activities, but broadly speaking it can be used to: find trends in a
dataset, classify data into groups or categories, make predictions based
upon data, and even “learn” how to interact with an environment when
provided with goals to achieve.</p>
<div class="section level3">
<h3 id="artificial-intelligence-vs-machine-learning">Artificial intelligence vs machine learning<a class="anchor" aria-label="anchor" href="#artificial-intelligence-vs-machine-learning"></a>
</h3>
<p>The term machine learning (ML) is often mentioned alongside
artificial intelligence (AI) and deep learning (DL). Deep learning is a
subset of machine learning, and machine learning is a subset of
artificial intelligence.</p>
<p>AI is increasingly being used as a catch-all term to describe things
that encompass ML and DL systems - from simple email spam filters, to
more complex image recognition systems, to large language models such as
ChatGPT. The more specific term “Artificial General Intelligence” (AGI)
is used to describe a system possessing a “general intelligence” that
can be applied to solve a diverse range of problems, often mimicking the
behaviour of intelligent biological systems. Modern attempts at AGI are
getting close to fooling humans, but while there have been great
advances in AI research, human-like intelligence is only possible in a
few specialist areas.</p>
<p>ML refers to techniques where a computer can “learn” patterns in
data, usually by being shown many training examples. While ML algorithms
can learn to solve specific problems, or multiple similar problems, they
are not considered to possess a general intelligence. ML algorithms
often need hundreds or thousands of examples to learn a task and are
confined to activities such as simple classifications. A human-like
system could learn much quicker than this, and potentially learn from a
single example by using it’s knowledge of many other problems.</p>
<p>DL is a particular field of machine learning where algorithms called
neural networks are used to create highly complex systems. Large
collections of neural networks are able to learn from vast quantities of
data. Deep learning can be used to solve a wide range of problems, but
it can also require huge amounts of input data and computational
resources to train.</p>
<p>The image below shows the relationships between artificial
intelligence, machine learning and deep learning.</p>
<p><img src="fig/introduction/AI_ML_DL_differences.png" alt="An infographic showing some of the relationships between AI, ML, and DL" class="figure">
The image above is by Tukijaaliwa, CC BY-SA 4.0, via Wikimedia Commons,
original source</p>
</div>
<div class="section level3">
<h3 id="machine-learning-in-our-daily-lives">Machine learning in our daily lives<a class="anchor" aria-label="anchor" href="#machine-learning-in-our-daily-lives"></a>
</h3>
<p>Machine learning has quickly become an important technology and is
now frequently used to perform services we encounter in our daily lives.
Here are just a few examples:</p>
<ul>
<li>Banks look for trends in transaction data to detect outliers that
may be fraudulent</li>
<li>Email inboxes use text to decide whether an email is spam or not,
and adjust their rules based upon how we flag emails</li>
<li>Travel apps use live and historic data to estimate traffic, travel
times, and journey routes</li>
<li>Retail companies and streaming services use data to recommend new
content we might like based upon our demographic and historical
preferences</li>
<li>Image, object, and pattern recognition is used to identify humans
and vehicles, capture text, generate subtitles, and much more</li>
<li>Self-driving cars and robots use object detection and performance
feedback to improve their interaction with the world</li>
</ul>
<div id="where-else-have-you-encountered-machine-learning-already" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="where-else-have-you-encountered-machine-learning-already" class="callout-inner">
<h3 class="callout-title">Where else have you encountered machine learning already?</h3>
<div class="callout-content">
<p>Now that we have explored machine learning in a bit more detail,
discuss with the person next to you:</p>
<ol style="list-style-type: decimal">
<li>Where else have I seen machine learning in use?</li>
<li>What kind of input data does that machine learning system use to
make predictions/classifications?</li>
<li>Is there any evidence that your interaction with the system
contributes to further training?</li>
<li>Do you have any examples of the system failing?</li>
</ol>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="limitations-of-machine-learning">Limitations of machine learning<a class="anchor" aria-label="anchor" href="#limitations-of-machine-learning"></a>
</h3>
<p>Like any other systems machine learning has limitations, caveats, and
“gotchas” to be aware of that may impact the accuracy and performance of
a machine learning system.</p>
<div class="section level4">
<h4 id="garbage-in-garbage-out">Garbage in = garbage out<a class="anchor" aria-label="anchor" href="#garbage-in-garbage-out"></a>
</h4>
<p>There is a classic expression in computer science: “garbage in =
garbage out”. This means that if the input data we use is garbage then
the ouput will be too. If, for example, we try to use a machine learning
system to find a link between two unlinked variables then it may well
manage to produce a model attempting this, but the output will be
meaningless.</p>
</div>
<div class="section level4">
<h4 id="biases-due-to-training-data">Biases due to training data<a class="anchor" aria-label="anchor" href="#biases-due-to-training-data"></a>
</h4>
<p>The performance of a ML system depends on the breadth and quality of
input data used to train it. If the input data contains biases or blind
spots then these will be reflected in the ML system. For example, if we
collect data on public transport use from only high socioeconomic areas,
the resulting input data may be biased due to a range of factors that
may increase the likelihood of people from those areas using private
transport vs public options.</p>
</div>
<div class="section level4">
<h4 id="extrapolation">Extrapolation<a class="anchor" aria-label="anchor" href="#extrapolation"></a>
</h4>
<p>We can only make reliable predictions about data which is in the same
range as our training data. If we try to extrapolate beyond the
boundaries of the training data we cannot be confident in our results.
As we shall see some algorithms are better suited (or less suited) to
extrapolation than others.</p>
</div>
<div class="section level4">
<h4 id="over-fitting">Over fitting<a class="anchor" aria-label="anchor" href="#over-fitting"></a>
</h4>
<p>Sometimes ML algorithms become over-trained and subsequently don’t
perform well when presented with real data. It’s important to consider
how many rounds of training a ML system has recieved and whether or not
it may have become over-trained.</p>
</div>
<div class="section level4">
<h4 id="inability-to-explain-answers">Inability to explain answers<a class="anchor" aria-label="anchor" href="#inability-to-explain-answers"></a>
</h4>
<p>Machine learning techniques will return an answer based on the input
data and model parameters even if that answer is wrong. Most systems are
unable to explain the logic used to arrive at that answer. This can make
detecting and diagnosing problems difficult.</p>
</div>
</div>
</section><section><h2 class="section-heading" id="getting-started-with-scikit-learn">Getting started with Scikit-Learn<a class="anchor" aria-label="anchor" href="#getting-started-with-scikit-learn"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="about-scikit-learn">About Scikit-Learn<a class="anchor" aria-label="anchor" href="#about-scikit-learn"></a>
</h3>
<p><a href="https://github.com/scikit-learn/scikit-learn" class="external-link">Scikit-Learn</a> is
a python package designed to give access to well-known machine learning
algorithms within Python code, through a clean application programming
interface (API). It has been built by hundreds of contributors from
around the world, and is used across industry and academia.</p>
<p>Scikit-Learn is built upon Python’s <a href="https://numpy.org/" class="external-link">NumPy (Numerical Python)</a> and <a href="https://scipy.org/" class="external-link">SciPy (Scientific Python)</a> libraries, which
enable efficient in-core numerical and scientific computation within
Python. As such, Scikit-Learn is not specifically designed for extremely
large datasets, though there is <a href="https://github.com/ogrisel/parallel_ml_tutorial" class="external-link">some work</a> in
this area. For this introduction to ML we are going to stick to
processing small to medium datasets with Scikit-Learn, without the need
for a graphical processing unit (GPU).</p>
<p>Like any other Python package, we can import Scikit-Learn and check
the package version using the following Python commands:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'scikit-learn:'</span>, sklearn.__version__)</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="representation-of-data-in-scikit-learn">Representation of Data in Scikit-learn<a class="anchor" aria-label="anchor" href="#representation-of-data-in-scikit-learn"></a>
</h3>
<p>Machine learning is about creating models from data: for that reason,
we’ll start by discussing how data can be represented in order to be
understood by the computer.</p>
<p>Most machine learning algorithms implemented in scikit-learn expect
data to be stored in a two-dimensional array or matrix. The arrays can
be either numpy arrays, or in some cases scipy.sparse matrices. The size
of the array is expected to be [n_samples, n_features]</p>
<p>We typically have a “Features Matrix” (usually referred to as the
code variable <code>X</code>) which are the “features” data we wish to
train on.</p>
<ul>
<li>n_samples: The number of samples. A sample can be a document, a
picture, a sound, a video, an astronomical object, a row in database or
CSV file, or whatever you can describe with a fixed set of quantitative
traits.</li>
<li>n_features: The number of features (variables) that can be used to
describe each item in a quantitative manner. Features are generally
real-valued, but may be boolean or discrete-valued in some cases.</li>
</ul>
<p>If we want our ML models to make predictions or classifications, we
also provide “labels” as our expected “answers/results”. The model will
then be trained on the input features to try and match our provided
labels. This is done by providing a “Target Array” (usually referred to
as the code variable <code>y</code>) which contains the “labels or
values” that we wish to predict using the features data.</p>
<p><img src="fig/introduction/sklearn_input.png" alt="A feature matrix, which contains N features and N samples (which is referred to as 'X'), and a target array, which contains N labels or values (referred to as 'y')." class="figure">
Figure from the <a href="https://github.com/jakevdp/PythonDataScienceHandbook" class="external-link">Python Data
Science Handbook</a></p>
</div>
</section><section><h2 class="section-heading" id="what-will-we-cover-today">What will we cover today?<a class="anchor" aria-label="anchor" href="#what-will-we-cover-today"></a>
</h2>
<hr class="half-width">
<p>This lesson will introduce you to some of the key concepts and
sub-domains of ML such as supervised learning, unsupervised learning,
and neural networks.</p>
<p>The figure below provides a nice overview of some of the sub-domains
of ML and the techniques used within each sub-domain. We recommend
checking out the Scikit-Learn <a href="https://scikit-learn.org/stable/index.html" class="external-link">webpage</a> for
additional examples of the topics we will cover in this lesson. We will
cover topics highlighted in blue: classical learning techniques such as
regression, classification, clustering, and dimension reduction, as well
as ensemble methods and a brief introduction to neural networks using
perceptrons.</p>
<p><img src="fig/introduction/ML_summary.png" alt="A cloud diagram showing the relationships between different types of machine learning and the techniques used within each type." class="figure"><a href="https://vas3k.com/blog/machine_learning/" class="external-link">Image from Vasily
Zubarev via their blog</a> with modifications in blue to denote lesson
content.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Machine learning is a set of tools and techniques that use data to
make predictions.</li>
<li>Artificial intelligence is a broader term that refers to making
computers show human-like intelligence.</li>
<li>Deep learning is a subset of machine learning.</li>
<li>All machine learning systems have limitations to be aware of.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-02-regression"><p>Content from <a href="02-regression.html">Supervised methods - Regression</a></p>
<hr>
<p>Last updated on 2025-07-23 |

        <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/edit/main/episodes/02-regression.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is supervised learning?</li>
<li>What is regression?</li>
<li>How can I model data and make predictions using regression
methods?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Apply linear regression with Scikit-Learn to create a model.</li>
<li>Measure the error between a regression model and input data.</li>
<li>Analyse and assess the accuracy of a linear model using
Scikit-Learn’s metrics library.</li>
<li>Understand how more complex models can be built with non-linear
equations.</li>
<li>Apply polynomial modelling to non-linear data using
Scikit-Learn.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="supervised-learning">Supervised learning<a class="anchor" aria-label="anchor" href="#supervised-learning"></a>
</h2>
<hr class="half-width">
<p>Classical machine learning is often divided into two categories –
supervised and unsupervised learning.</p>
<p>For the case of supervised learning we act as a “supervisor” or
“teacher” for our ML algorithms by providing the algorithm with
“labelled data” that contains example answers of what we wish the
algorithm to achieve.</p>
<p>For instance, if we wish to train our algorithm to distinguish
between images of cats and dogs, we would provide our algorithm with
images that have already been labelled as “cat” or “dog” so that it can
learn from these examples. If we wished to train our algorithm to
predict house prices over time we would provide our algorithm with
example data of datetime values that are “labelled” with house
prices.</p>
<p>Supervised learning is split up into two further categories:
classification and regression. For classification the labelled data is
discrete, such as the “cat” or “dog” example, whereas for regression the
labelled data is continuous, such as the house price example.</p>
<p>In this episode we will explore how we can use regression to build a
“model” that can be used to make predictions.</p>
</section><section><h2 class="section-heading" id="regression">Regression<a class="anchor" aria-label="anchor" href="#regression"></a>
</h2>
<hr class="half-width">
<p>Regression is a statistical technique that relates a dependent
variable (a label in ML terms) to one or more independent variables
(features in ML terms). A regression model attempts to describe this
relation by fitting the data as closely as possible according to
mathematical criteria. This model can then be used to predict new
labelled values by inputting the independent variables into it. For
example, if we create a house price model we can then feed in any
datetime value we wish, and get a new house price value prediction.</p>
<p>Regression can be as simple as drawing a “line of best fit” through
data points, known as linear regression, or more complex models such as
polynomial regression, and is used routinely around the world in both
industry and research. You may have already used regression in the past
without knowing that it is also considered a machine learning
technique!</p>
<figure><img src="fig/regression_example.png" alt="A pair of graphs comparing a linear and a polynomial regression. The linear regression is a straight line that fits through the data points, while the polynomial regression is a curved line that fits more closely to the data points." class="figure mx-auto d-block"><div class="figcaption">Example of linear and polynomial
regressions</div>
</figure><div class="section level3">
<h3 id="linear-regression-using-scikit-learn">Linear regression using Scikit-Learn<a class="anchor" aria-label="anchor" href="#linear-regression-using-scikit-learn"></a>
</h3>
<p>We’ve had a lot of theory so time to start some actual coding! Let’s
create regression models for a small bundle of datasets known as <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet" class="external-link">Anscombe’s
Quartet</a>. These datasets are available through the Python plotting
library <a href="https://seaborn.pydata.org/" class="external-link">Seaborn</a>. Let’s define
our bundle of datasets, extract out the first dataset, and inspect it’s
contents:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># Anscomes Quartet consists of 4 sets of data</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>data <span class="op">=</span> sns.load_dataset(<span class="st">"anscombe"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>data.head()</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># Split out the 1st dataset from the total</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>data_1 <span class="op">=</span> data[data[<span class="st">"dataset"</span>]<span class="op">==</span><span class="st">"I"</span>]</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>data_1 <span class="op">=</span> data_1.sort_values(<span class="st">"x"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co"># Inspect the data</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>data_1.head()</span></code></pre>
</div>
<p>We see that the dataset bundle has the 3 columns
<code>dataset</code>, <code>x</code>, and <code>y</code>. We have
already used the <code>dataset</code> column to extract out Dataset I
ready for our regression task. Let’s visually inspect the data:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>plt.scatter(data_1[<span class="st">"x"</span>], data_1[<span class="st">"y"</span>])</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/regression_inspect.png" alt="A scatter plot of the first Anscombe dataset." class="figure mx-auto d-block"><div class="figcaption">Inspection of our dataset</div>
</figure><p>In this regression example we will create a Linear Regression model
that will try to predict <code>y</code> values based upon <code>x</code>
values.</p>
<p>In machine learning terminology: we will use our <code>x</code>
feature (variable) and <code>y</code> labels(“answers”) to train our
Linear Regression model to predict <code>y</code> values when provided
with <code>x</code> values.</p>
<p>The mathematical equation for a linear fit is <code>y = mx + c</code>
where <code>y</code> is our label data, <code>x</code> is our input
feature(s), <code>m</code> represents the gradient of the linear fit,
and <code>c</code> represents the intercept with the y-axis.</p>
<p>A typical ML workflow is as following: * Define the model (also known
as an estimator) * Tweak your data into the required format for your
model * Train your model on the input data * Predict some values using
the trained model * Check the accuracy of the prediction, and visualise
the result</p>
<p>We’ll define functions for each of these steps so that we can quickly
perform linear regressions on our data. First we’ll define a function to
pre-process our data into a format that Scikit-Learn can use.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="kw">def</span> pre_process_linear(x, y):</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    <span class="co"># sklearn requires a 2D array, so lets reshape our 1D arrays.</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    x_data <span class="op">=</span> np.array(x).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    y_data <span class="op">=</span> np.array(y).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    <span class="cf">return</span> x_data, y_data</span></code></pre>
</div>
<p>Next we’ll define a model, and train it on the pre-processed data.
We’ll also inspect the trained model parameters <code>m</code> and
<code>c</code>:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="kw">def</span> fit_a_linear_model(x_data, y_data):</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    <span class="co"># Define our estimator/model</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    model <span class="op">=</span> LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    <span class="co"># train our estimator/model using our data</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    lin_regress <span class="op">=</span> model.fit(x_data, y_data)</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>    <span class="co"># inspect the trained estimator/model parameters</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>    m <span class="op">=</span> lin_regress.coef_</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>    c <span class="op">=</span> lin_regress.intercept_</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"linear coefs="</span>, m, c)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>    <span class="cf">return</span> lin_regress</span></code></pre>
</div>
<p>Then we’ll define a function to make predictions using our trained
model, and calculate the Root Mean Squared Error (RMSE) of our
predictions:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="kw">def</span> predict_linear_model(lin_regress, x_data, y_data):</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    <span class="co"># predict some values using our trained estimator/model</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    <span class="co"># (in this case we predict our input data!)</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    linear_data <span class="op">=</span> lin_regress.predict(x_data)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>    <span class="co"># calculated a RMS error as a quality of fit metric</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>    error <span class="op">=</span> math.sqrt(mean_squared_error(y_data, linear_data))</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"linear error="</span>, error)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>    <span class="co"># return our trained model so that we can use it later</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    <span class="cf">return</span> linear_data</span></code></pre>
</div>
<p>Finally, we’ll define a function to plot our input data, our linear
fit, and our predictions:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> plot_linear_model(x_data, y_data, predicted_data):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="co"># visualise!</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    <span class="co"># Don't call .show() here so that we can add extra stuff to the figure later</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    plt.scatter(x_data, y_data, label<span class="op">=</span><span class="st">"input"</span>)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    plt.plot(x_data, predicted_data, <span class="st">"-"</span>, label<span class="op">=</span><span class="st">"fit"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    plt.plot(x_data, predicted_data, <span class="st">"rx"</span>, label<span class="op">=</span><span class="st">"predictions"</span>)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>    plt.legend()</span></code></pre>
</div>
<p>We will be training a few Linear Regression models in this episode,
so let’s define a handy function to combine input data processing, model
creation, training our model, inspecting the trained model parameters
<code>m</code> and <code>c</code>, make some predictions, and finally
visualise our data.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> fit_predict_plot_linear(x, y):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    x_data, y_data <span class="op">=</span> pre_process_linear(x, y)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    lin_regress <span class="op">=</span> fit_a_linear_model(x_data, y_data)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    linear_data <span class="op">=</span> predict_linear_model(lin_regress, x_data, y_data)</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    plot_linear_model(x_data, y_data, linear_data)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    <span class="cf">return</span> lin_regress</span></code></pre>
</div>
<p>Now we have defined our generic function to fit a linear regression
we can call the function to train it on some data, and show the plot
that was generated:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># just call the function here rather than assign.</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="co"># We don't need to reuse the trained model yet</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>fit_predict_plot_linear(data_1[<span class="st">"x"</span>], data_1[<span class="st">"y"</span>])</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/regress_linear.png" alt="A plot of the first Anscombe dataset with a linear regression line fitted to the data points." class="figure mx-auto d-block"><div class="figcaption">Linear regression of dataset I</div>
</figure><p>This looks like a reasonable linear fit to our first dataset. Thanks
to our function we can quickly perform more linear regressions on other
datasets.</p>
<p>Let’s quickly perform a new linear fit on the 2nd Anscombe
dataset:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>data_2 <span class="op">=</span> data[data[<span class="st">"dataset"</span>]<span class="op">==</span><span class="st">"II"</span>]</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>fit_predict_plot_linear(data_2[<span class="st">"x"</span>],data_2[<span class="st">"y"</span>])</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/regress_linear_2nd.png" alt="A plot of the second Anscombe dataset with a linear regression line fitted to the data points. The points are not well aligned with the linear regression line." class="figure mx-auto d-block"><div class="figcaption">Linear regression of dataset II</div>
</figure><p>It looks like our linear fit on Dataset II produces a nearly
identical fit to the linear fit on Dataset I. Although our errors look
to be almost identical our visual inspection tells us that Dataset II is
probably not a linear correllation and we should try to make a different
model.</p>
<div id="exercise-repeat-the-linear-regression-excercise-for-datasets-iii-and-iv." class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-repeat-the-linear-regression-excercise-for-datasets-iii-and-iv." class="callout-inner">
<h3 class="callout-title">Exercise: Repeat the linear regression excercise for Datasets III and IV.</h3>
<div class="callout-content">
<p>Adjust your code to repeat the linear regression for the other
datasets. What can you say about the similarities and/or differences
between the linear regressions on the 4 datasets?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># Repeat the following and adjust for dataset IV</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>data_3 <span class="op">=</span> data[data[<span class="st">"dataset"</span>]<span class="op">==</span><span class="st">"III"</span>]</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>fit_predict_plot_linear(data_3[<span class="st">"x"</span>],data_3[<span class="st">"y"</span>])</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/regress_linear_3rd.png" alt="A plot of the third Anscombe dataset with a linear regression line fitted to the data points. The points are not well aligned with the linear regression line." class="figure mx-auto d-block"><div class="figcaption">Linear regression of dataset III</div>
</figure><figure><img src="fig/regress_linear_4th.png" alt="A plot of the fourth Anscombe dataset with a linear regression line fitted to the data points. The points are not well aligned with the linear regression line." class="figure mx-auto d-block"><div class="figcaption">Linear regression of dataset IV</div>
</figure><p>The 4 datasets all produce very similar linear regression fit
parameters (<code>m</code> and <code>c</code>) and RMSEs despite visual
differences in the 4 datasets. This is intentional as the Anscombe
Quartet is designed to produce near identical basic statistical values
such as means and standard deviations. While the trained model
parameters and errors are near identical, our visual inspection tells us
that a linear fit might not be the best way of modelling all of these
datasets.</p>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="polynomial-regression-using-scikit-learn">Polynomial regression using Scikit-Learn<a class="anchor" aria-label="anchor" href="#polynomial-regression-using-scikit-learn"></a>
</h2>
<hr class="half-width">
<p>Now that we have learnt how to do a linear regression it’s time look
into polynomial regressions. Polynomial functions are non-linear
functions that are commonly-used to model data. Mathematically they have
<code>N</code> degrees of freedom and they take the following form
<code>y = a + bx + cx^2 + dx^3 ... + mx^N</code></p>
<p>If we have a polynomial of degree N=1 we once again return to a
linear equation <code>y = a + bx</code> or as it is more commonly
written <code>y = mx + c</code>. Let’s create a polynomial regression
using N=2.</p>
<p>In Scikit-Learn this is done in two steps. First we pre-process our
input data <code>x_data</code> into a polynomial representation using
the <code>PolynomialFeatures</code> function. Then we can create our
polynomial regressions using the <code>LinearRegression().fit()</code>
function, but this time using the polynomial representation of our
<code>x_data</code>.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="kw">def</span> pre_process_poly(x, y):</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    <span class="co"># sklearn requires a 2D array, so lets reshape our 1D arrays.</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>    x_data <span class="op">=</span> np.array(x).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    y_data <span class="op">=</span> np.array(y).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>    <span class="co"># create a polynomial representation of our data</span></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>    poly_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>    x_poly <span class="op">=</span> poly_features.fit_transform(x_data)</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>    <span class="cf">return</span> x_poly, x_data, y_data</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a><span class="kw">def</span> fit_poly_model(x_poly, y_data):</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a>    <span class="co"># Define our estimator/model(s)</span></span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a>    poly_regress <span class="op">=</span> LinearRegression()</span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>    <span class="co"># define and train our model</span></span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a>    poly_regress.fit(x_poly, y_data)</span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a>    <span class="co"># inspect trained model parameters</span></span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>    poly_m <span class="op">=</span> poly_regress.coef_</span>
<span id="cb11-24"><a href="#cb11-24" tabindex="-1"></a>    poly_c <span class="op">=</span> poly_regress.intercept_</span>
<span id="cb11-25"><a href="#cb11-25" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"poly_coefs"</span>, poly_m, poly_c)</span>
<span id="cb11-26"><a href="#cb11-26" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" tabindex="-1"></a>    <span class="cf">return</span> poly_regress</span>
<span id="cb11-28"><a href="#cb11-28" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" tabindex="-1"></a><span class="kw">def</span> predict_poly_model(poly_regress, x_poly, y_data):</span>
<span id="cb11-31"><a href="#cb11-31" tabindex="-1"></a>    <span class="co"># predict some values using our trained estimator/model</span></span>
<span id="cb11-32"><a href="#cb11-32" tabindex="-1"></a>    <span class="co"># (in this case - our input data)</span></span>
<span id="cb11-33"><a href="#cb11-33" tabindex="-1"></a>    poly_data <span class="op">=</span> poly_regress.predict(x_poly)</span>
<span id="cb11-34"><a href="#cb11-34" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" tabindex="-1"></a>    poly_error <span class="op">=</span> math.sqrt(mean_squared_error(y_data, poly_data))</span>
<span id="cb11-36"><a href="#cb11-36" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"poly error="</span>, poly_error)</span>
<span id="cb11-37"><a href="#cb11-37" tabindex="-1"></a></span>
<span id="cb11-38"><a href="#cb11-38" tabindex="-1"></a>    <span class="cf">return</span> poly_data</span>
<span id="cb11-39"><a href="#cb11-39" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" tabindex="-1"></a></span>
<span id="cb11-41"><a href="#cb11-41" tabindex="-1"></a><span class="kw">def</span> plot_poly_model(x_data, poly_data):</span>
<span id="cb11-42"><a href="#cb11-42" tabindex="-1"></a>    <span class="co"># visualise!</span></span>
<span id="cb11-43"><a href="#cb11-43" tabindex="-1"></a>    plt.plot(x_data, poly_data, label<span class="op">=</span><span class="st">"poly fit"</span>)</span>
<span id="cb11-44"><a href="#cb11-44" tabindex="-1"></a>    plt.legend()</span>
<span id="cb11-45"><a href="#cb11-45" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" tabindex="-1"></a></span>
<span id="cb11-47"><a href="#cb11-47" tabindex="-1"></a><span class="kw">def</span> fit_predict_plot_poly(x, y):</span>
<span id="cb11-48"><a href="#cb11-48" tabindex="-1"></a>    <span class="co"># Combine all of the steps</span></span>
<span id="cb11-49"><a href="#cb11-49" tabindex="-1"></a>    x_poly, x_data, y_data <span class="op">=</span> pre_process_poly(x, y)</span>
<span id="cb11-50"><a href="#cb11-50" tabindex="-1"></a>    poly_regress <span class="op">=</span> fit_poly_model(x_poly, y_data)</span>
<span id="cb11-51"><a href="#cb11-51" tabindex="-1"></a>    poly_data <span class="op">=</span> predict_poly_model(poly_regress, x_poly, y_data)</span>
<span id="cb11-52"><a href="#cb11-52" tabindex="-1"></a>    plot_poly_model(x_data, poly_data)</span>
<span id="cb11-53"><a href="#cb11-53" tabindex="-1"></a></span>
<span id="cb11-54"><a href="#cb11-54" tabindex="-1"></a>    <span class="cf">return</span> poly_regress</span></code></pre>
</div>
<p>Lets plot our input dataset II, linear model, and polynomial model
together, as well as compare the errors of the linear and polynomial
fits.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># Sort our data in order of our x (feature) values</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>data_2 <span class="op">=</span> data[data[<span class="st">"dataset"</span>] <span class="op">==</span> <span class="st">"II"</span>]</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>data_2 <span class="op">=</span> data_2.sort_values(<span class="st">"x"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>fit_predict_plot_linear(data_2[<span class="st">"x"</span>], data_2[<span class="st">"y"</span>])</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>fit_predict_plot_poly(data_2[<span class="st">"x"</span>], data_2[<span class="st">"y"</span>])</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/regress_both.png" alt="The second Anscombe dataset with a linear regression line and a polynomial regression line fitted to the data points. The polynomial regression fits the data points much more closely than the linear regression." class="figure mx-auto d-block"><div class="figcaption">Comparison of the regressions of our
dataset</div>
</figure><p>Comparing the plots and errors it seems like a polynomial regression
of N=2 is a far superior fit to Dataset II than a linear fit. In fact,
it looks like our polynomial fit almost perfectly fits Dataset II… which
is because Dataset II is created from a N=2 polynomial equation!</p>
<div id="exercise-perform-and-compare-linear-and-polynomial-fits-for-datasets-i-ii-iii-and-iv." class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-perform-and-compare-linear-and-polynomial-fits-for-datasets-i-ii-iii-and-iv." class="callout-inner">
<h3 class="callout-title">Exercise: Perform and compare linear and polynomial fits for Datasets I, II, III, and IV.</h3>
<div class="callout-content">
<p>Which performs better for each dataset? Modify your polynomial
regression function to take <code>N</code> as an input parameter to your
regression model. How does changing the degree of polynomial fit affect
each dataset?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="cf">for</span> ds <span class="kw">in</span> [<span class="st">"I"</span>, <span class="st">"II"</span>, <span class="st">"III"</span>, <span class="st">"IV"</span>]:</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>    <span class="co"># Sort our data in order of our x (feature) values</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>    data_ds <span class="op">=</span> data[data[<span class="st">"dataset"</span>] <span class="op">==</span> ds]</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    data_ds <span class="op">=</span> data_ds.sort_values(<span class="st">"x"</span>)</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    fit_predict_plot_linear(data_ds[<span class="st">"x"</span>], data_ds[<span class="st">"y"</span>])</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>    fit_predict_plot_poly(data_ds[<span class="st">"x"</span>], data_ds[<span class="st">"y"</span>])</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>    plt.show()</span></code></pre>
</div>
<p><img src="fig/regress_polynomial_1st.png" alt="A plot of the first Anscombe dataset with a polynomial regression line fitted to the data points." class="figure"><img src="fig/regress_polynomial_2nd.png" alt="A plot of the second Anscombe dataset with a polynomial regression line fitted to the data points." class="figure"><img src="fig/regress_polynomial_3rd.png" alt="A plot of the third Anscombe dataset with a polynomial regression line fitted to the data points." class="figure"><img src="fig/regress_polynomial_4th.png" alt="A plot of the fourth Anscombe dataset with a polynomial regression line fitted to the data points." class="figure"></p>
<p>The <code>N=2</code> polynomial fit is far better for Dataset II.
According to the RMSE the polynomial is a slightly better fit for
Datasets I and III, however it could be argued that a linear fit is good
enough. Dataset III looks like a linear relation that has a single
outlier, rather than a truly non-linear relation. The polynomial and
linear fits perform just as well (or poorly) on Dataset IV. For Dataset
IV it looks like <code>y</code> may be a better estimator of
<code>x</code>, than <code>x</code> is at estimating <code>y</code>.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="kw">def</span> pre_process_poly(x, y, N):</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>    <span class="co"># sklearn requires a 2D array, so lets reshape our 1D arrays.</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>    x_data <span class="op">=</span> np.array(x).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>    y_data <span class="op">=</span> np.array(y).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>    <span class="co"># create a polynomial representation of our data</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>    poly_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>N)</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>    x_poly <span class="op">=</span> poly_features.fit_transform(x_data)</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>    <span class="cf">return</span> x_poly, x_data, y_data</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a><span class="kw">def</span> plot_poly_model(x_data, poly_data, N):</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>    <span class="co"># visualise!</span></span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>    plt.plot(x_data, poly_data, label<span class="op">=</span><span class="st">"poly fit N="</span> <span class="op">+</span> <span class="bu">str</span>(N))</span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>    plt.legend()</span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a><span class="kw">def</span> fit_predict_plot_poly(x, y, N):</span>
<span id="cb14-18"><a href="#cb14-18" tabindex="-1"></a>    <span class="co"># Combine all of the steps</span></span>
<span id="cb14-19"><a href="#cb14-19" tabindex="-1"></a>    x_poly, x_data, y_data <span class="op">=</span> pre_process_poly(x, y, N)</span>
<span id="cb14-20"><a href="#cb14-20" tabindex="-1"></a>    poly_regress <span class="op">=</span> fit_poly_model(x_poly, y_data)</span>
<span id="cb14-21"><a href="#cb14-21" tabindex="-1"></a>    poly_data <span class="op">=</span> predict_poly_model(poly_regress, x_poly, y_data)</span>
<span id="cb14-22"><a href="#cb14-22" tabindex="-1"></a>    plot_poly_model(x_data, poly_data, N)</span>
<span id="cb14-23"><a href="#cb14-23" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" tabindex="-1"></a>    <span class="cf">return</span> poly_regress</span>
<span id="cb14-25"><a href="#cb14-25" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" tabindex="-1"></a><span class="cf">for</span> ds <span class="kw">in</span> [<span class="st">"I"</span>, <span class="st">"II"</span>, <span class="st">"III"</span>, <span class="st">"IV"</span>]:</span>
<span id="cb14-27"><a href="#cb14-27" tabindex="-1"></a>    <span class="co"># Sort our data in order of our x (feature) values</span></span>
<span id="cb14-28"><a href="#cb14-28" tabindex="-1"></a>    data_ds <span class="op">=</span> data[data[<span class="st">"dataset"</span>] <span class="op">==</span> ds]</span>
<span id="cb14-29"><a href="#cb14-29" tabindex="-1"></a>    data_ds <span class="op">=</span> data_ds.sort_values(<span class="st">"x"</span>)</span>
<span id="cb14-30"><a href="#cb14-30" tabindex="-1"></a>    fit_predict_plot_linear(data_ds[<span class="st">"x"</span>], data_ds[<span class="st">"y"</span>])</span>
<span id="cb14-31"><a href="#cb14-31" tabindex="-1"></a>    <span class="cf">for</span> N <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">11</span>):</span>
<span id="cb14-32"><a href="#cb14-32" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Polynomial degree ="</span>,N)</span>
<span id="cb14-33"><a href="#cb14-33" tabindex="-1"></a>        fit_predict_plot_poly(data_ds[<span class="st">"x"</span>], data_ds[<span class="st">"y"</span>],N)</span>
<span id="cb14-34"><a href="#cb14-34" tabindex="-1"></a>    plt.show()</span></code></pre>
</div>
<p>and</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="cf">for</span> ds <span class="kw">in</span> [<span class="st">"I"</span>, <span class="st">"II"</span>, <span class="st">"III"</span>, <span class="st">"IV"</span>]:</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>    <span class="co"># Sort our data in order of our x (feature) values</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>    data_ds <span class="op">=</span> data[data[<span class="st">"dataset"</span>] <span class="op">==</span> ds]</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>    data_ds <span class="op">=</span> data_ds.sort_values(<span class="st">"x"</span>)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>    fit_predict_plot_linear(data_ds[<span class="st">"x"</span>], data_ds[<span class="st">"y"</span>])</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>    <span class="cf">for</span> N <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">11</span>):</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Polynomial degree ="</span>, N)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>        fit_predict_plot_poly(data_ds[<span class="st">"x"</span>], data_ds[<span class="st">"y"</span>], N)</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>    plt.show()</span></code></pre>
</div>
<p>With a large enough polynomial you can fit through every point with a
unique <code>x</code> value. Datasets II and IV remain unchanged beyond
<code>N=2</code> as the polynomial has converged (dataset II) or cannot
model the data (Dataset IV). Datasets I and III slowly decrease their
RMSE and N is increased, but it is likely that these more complex models
are overfitting the data. Overfitting is discussed later in the
lesson.</p>
<p><img src="fig/regress_polynomial_n_1st.png" alt="A plot of the first Anscombe dataset with polynomial regression lines fitted to the data points for polynomial degrees N=1 to N=10. The polynomial lines become more complex as N increases." class="figure"><img src="fig/regress_polynomial_n_2nd.png" alt="A plot of the second Anscombe dataset with polynomial regression lines fitted to the data points for polynomial degrees N=1 to N=10. All of the polynomial lines are very similar and overlap the data points." class="figure"><img src="fig/regress_polynomial_n_3rd.png" alt="A plot of the third Anscombe dataset with polynomial regression lines fitted to the data points for polynomial degrees N=1 to N=10." class="figure"><img src="fig/regress_polynomial_n_4th.png" alt="A plot of the fourth Anscombe dataset with polynomial regression lines fitted to the data points for polynomial degrees N=1 to N=10. All of the polynomial lines are similar and overlap." class="figure"></p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="lets-explore-a-more-realistic-scenario">Let’s explore a more realistic scenario<a class="anchor" aria-label="anchor" href="#lets-explore-a-more-realistic-scenario"></a>
</h2>
<hr class="half-width">
<p>Now that we have some convenient Python functions to perform quick
regressions on data it’s time to explore a more realistic regression
modelling scenario.</p>
<p>Let’s start by loading in and examining a new dataset from Seaborn: a
penguin dataset containing a few hundred samples and a number of
features and labels.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>dataset <span class="op">=</span> sns.load_dataset(<span class="st">"penguins"</span>)</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>dataset.head()</span></code></pre>
</div>
<p>We can see that we have seven columns in total: 4 continuous
(numerical) columns named <code>bill_length_mm</code>,
<code>bill_depth_mm</code>, <code>flipper_length_mm</code>, and
<code>body_mass_g</code>; and 3 discrete (categorical) columns named
<code>species</code>, <code>island</code>, and <code>sex</code>. We can
also see from a quick inspection of the first 5 samples that we have
some missing data in the form of <code>NaN</code> values. Let’s go ahead
and remove any rows that contain <code>NaN</code> values:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>dataset.dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>dataset.head()</span></code></pre>
</div>
<p>Now that we have cleaned our data we can try and predict a penguins
bill depth using their body mass. In this scenario we will train a
linear regression model using <code>body_mass_g</code> as our feature
data and <code>bill_depth_mm</code> as our label data. We will train our
model on a subset of the data by slicing the first 146 samples of our
cleaned data. We will then use our regression function to train and plot
our model.</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>dataset_1 <span class="op">=</span> dataset[:<span class="dv">146</span>]</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>x_data <span class="op">=</span> dataset_1[<span class="st">"body_mass_g"</span>]</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>y_data <span class="op">=</span> dataset_1[<span class="st">"bill_depth_mm"</span>]</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>trained_model <span class="op">=</span> fit_predict_plot_linear(x_data, y_data)</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>plt.xlabel(<span class="st">"mass g"</span>)</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>plt.ylabel(<span class="st">"depth mm"</span>)</span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/regress_penguin_lin.png" alt="A plot of a subset of the penguin dataset with a linear regression line fitted to the data points. The points are well aligned with the linear regression line." class="figure mx-auto d-block"><div class="figcaption">Comparison of the regressions of our
dataset</div>
</figure><p>Congratulations! We’ve taken our linear regression function and
quickly created and trained a new linear regression model on a brand new
dataset. Note that this time we have returned our model from the
regression function and assigned it to the variable
<code>trained_model</code>. We can now use this model to predict
<code>bill_depth_mm</code> values for any given <code>body_mass_g</code>
values that we pass it.</p>
<p>Let’s provide the model with all of the penguin samples and visually
inspect how the linear regression model performs.</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>x_data_all, y_data_all <span class="op">=</span> pre_process_linear(dataset[<span class="st">"body_mass_g"</span>], dataset[<span class="st">"bill_depth_mm"</span>])</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>y_predictions <span class="op">=</span> predict_linear_model(trained_model, x_data_all, y_data_all)</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>plt.scatter(x_data_all, y_data_all, label<span class="op">=</span><span class="st">"all data"</span>)</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>plt.scatter(x_data, y_data, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a>plt.plot(x_data_all, y_predictions, label<span class="op">=</span><span class="st">"fit"</span>)</span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a>plt.plot(x_data_all, y_predictions, <span class="st">"rx"</span>, label<span class="op">=</span><span class="st">"predictions"</span>)</span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" tabindex="-1"></a>plt.xlabel(<span class="st">"mass g"</span>)</span>
<span id="cb19-12"><a href="#cb19-12" tabindex="-1"></a>plt.ylabel(<span class="st">"depth mm"</span>)</span>
<span id="cb19-13"><a href="#cb19-13" tabindex="-1"></a>plt.legend()</span>
<span id="cb19-14"><a href="#cb19-14" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/regress_penguin_lin_tot.png" alt="A plot of the same regression, this time with the entire dataset. It is now clear that the lonear regression fits two classes, but is a poor predictor of the third class of penguins." class="figure mx-auto d-block"><div class="figcaption">Comparison of the regressions of our
dataset</div>
</figure><p>Oh dear. It looks like our linear regression fits okay for our subset
of the penguin data, and a few additional samples, but there appears to
be a cluster of points that are poorly predicted by our model.</p>
<div id="this-is-a-classic-machine-learning-scenario-known-as-over-fitting" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="this-is-a-classic-machine-learning-scenario-known-as-over-fitting" class="callout-inner">
<h3 class="callout-title">This is a classic Machine Learning scenario known as over-fitting</h3>
<div class="callout-content">
<p>We have trained our model on a specific set of data, and our model
has learnt to reproduce those specific answers at the expense of
creating a more generally-applicable model. Over fitting is the ML
equivalent of learning an exam papers mark scheme off by heart, rather
than understanding and answering the questions.</p>
</div>
</div>
</div>
<p>Perhaps our model is too simple? Perhaps our data is more complex
than we thought? Perhaps our question/goal needs adjusting? Let’s
explore the penguin dataset in more depth in the next section!</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Scikit-Learn is a Python library with lots of useful machine
learning functions.</li>
<li>Scikit-Learn includes a linear regression function.</li>
<li>Scikit-Learn can perform polynomial regressions to model non-linear
data.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-03-classification"><p>Content from <a href="03-classification.html">Supervised methods - Classification</a></p>
<hr>
<p>Last updated on 2025-07-23 |

        <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/edit/main/episodes/03-classification.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I classify data into known categories?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Use two different supervised methods to classify data.</li>
<li>Learn about the concept of hyper-parameters.</li>
<li>Learn to validate and cross-validate models</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="classification">Classification<a class="anchor" aria-label="anchor" href="#classification"></a>
</h2>
<hr class="half-width">
<p>Classification is a supervised method to recognise and group data
objects into a pre-determined categories. Where regression uses labelled
observations to predict a continuous numerical value, classification
predicts a discrete categorical fit to a class. Classification in ML
leverages a wide range of algorithms to classify a set of data/datasets
into their respective categories.</p>
<p>In this episode we are going to introduce the concept of supervised
classification by classifying penguin data into different species of
penguins using Scikit-Learn.</p>
</section><section><h2 class="section-heading" id="the-penguins-dataset">The penguins dataset<a class="anchor" aria-label="anchor" href="#the-penguins-dataset"></a>
</h2>
<hr class="half-width">
<p>We’re going to be using the <a href="https://github.com/allisonhorst/palmerpenguins" class="external-link">Palmer Penguins
dataset</a> of Allison Horst, The dataset contains 344 size measurements
for three penguin species (Chinstrap, Gentoo and Adélie) observed on
three islands in the Palmer Archipelago, Antarctica.</p>
<figure><img src="fig/palmer_penguins.png" alt="A cartoon image of three penguins standing on a snowy island. The penguins are labelled as Chinstrap, Gentoo and Adélie." class="figure mx-auto d-block"><div class="figcaption"><em>Artwork by <span class="citation">@allison_horst</span></em></div>
</figure><p>The physical attributes measured are flipper length, beak length,
beak width, body mass, and sex. <img src="fig/culmen_depth.png" alt="A drawn image of a penguin's head, showing where bill length and bill depth are measured." class="figure"></p>
<p>In other words, the dataset contains 344 rows with 7 features i.e. 5
physical attributes, species and the island where the observations were
made.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>dataset <span class="op">=</span> sns.load_dataset(<span class="st">'penguins'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>dataset.head()</span></code></pre>
</div>
<p>Our aim is to develop a classification model that will predict the
species of a penguin based upon measurements of those variables.</p>
<p>As a rule of thumb for ML/DL modelling, it is best to start with a
simple model and progressively add complexity in order to meet our
desired classification performance.</p>
<p>For this lesson we will limit our dataset to only numerical values
such as bill_length, bill_depth, flipper_length, and body_mass while we
attempt to classify species.</p>
<p>The above table contains multiple categorical objects such as
species. If we attempt to include the other categorical fields, island
and sex, we might hinder classification performance due to the
complexity of the data.</p>
<div class="section level3">
<h3 id="preprocessing-our-data">Preprocessing our data<a class="anchor" aria-label="anchor" href="#preprocessing-our-data"></a>
</h3>
<p>Lets do some pre-processing on our dataset and specify our
<code>X</code> features and <code>y</code> labels:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Extract the data we need</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>feature_names <span class="op">=</span> [<span class="st">'bill_length_mm'</span>, <span class="st">'bill_depth_mm'</span>, <span class="st">'flipper_length_mm'</span>, <span class="st">'body_mass_g'</span>]</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>dataset.dropna(subset<span class="op">=</span>feature_names, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>class_names <span class="op">=</span> dataset[<span class="st">'species'</span>].unique()</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>X <span class="op">=</span> dataset[feature_names]</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>y <span class="op">=</span> dataset[<span class="st">'species'</span>]</span></code></pre>
</div>
<p>Having extracted our features <code>X</code> and labels
<code>y</code>, we can now split the data using the
<code>train_test_split</code> function.</p>
</div>
</section><section><h2 class="section-heading" id="training-testing-split">Training-testing split<a class="anchor" aria-label="anchor" href="#training-testing-split"></a>
</h2>
<hr class="half-width">
<p>When undertaking any machine learning project, it’s important to be
able to evaluate how well your model works.</p>
<p>Rather than evaluating this manually we can instead set aside some of
our training data, usually 20% of our training data, and use these as a
testing dataset. We then train on the remaining 80% and use the testing
dataset to evaluate the accuracy of our trained model.</p>
<p>We lose a bit of training data in the process, But we can now easily
evaluate the performance of our model. With more advanced test-train
split techniques we can even recover this lost training data!</p>
<div id="why-do-we-do-this" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="why-do-we-do-this" class="callout-inner">
<h3 class="callout-title">Why do we do this?</h3>
<div class="callout-content">
<p>It’s important to do this early, and to do all of your work with the
training dataset - this avoids any risk of you introducing bias to the
model based on your own manual observations of data in the testing set
(afterall, we want the model to make the decisions about parameters!).
This can also highlight when you are over-fitting on your training
data.</p>
</div>
</div>
</div>
<p>How we split the data into training and testing sets is also
extremely important. We need to make sure that our training data is
representitive of both our test data and actual data.</p>
<p>For classification problems this means we should ensure that each
class of interest is represented proportionately in both training and
testing sets. For regression problems we should ensure that our training
and test sets cover the range of feature values that we wish to
predict.</p>
<p>In the previous regression episode we created the penguin training
data by taking the first 146 samples our the dataset. Unfortunately the
penguin data is sorted by species and so our training data only
considered one type of penguin and thus was not representitive of the
actual data we tried to fit. We could have avoided this issue by
randomly shuffling our penguin samples before splitting the data.</p>
<div id="when-not-to-shuffle-your-data" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="when-not-to-shuffle-your-data" class="callout-inner">
<h3 class="callout-title">When not to shuffle your data</h3>
<div class="callout-content">
<p>Sometimes your data is dependant on it’s ordering, such as
time-series data where past values influence future predictions.
Creating train-test splits for this can be tricky at first glance, but
fortunately there are existing techniques to tackle this (often called
stratification): See <a href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators" class="external-link">Scikit-Learn</a>
for more information.</p>
</div>
</div>
</div>
<p>We specify the fraction of data to use as test data, and the function
randomly shuffles our data prior to splitting:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span></code></pre>
</div>
<p>We’ll use <code>X_train</code> and <code>y_train</code> to develop
our model, and only look at <code>X_test</code> and <code>y_test</code>
when it’s time to evaluate its performance.</p>
<div class="section level3">
<h3 id="visualising-the-data">Visualising the data<a class="anchor" aria-label="anchor" href="#visualising-the-data"></a>
</h3>
<p>In order to better understand how a model might classify this data,
we can first take a look at the data visually, to see what patterns we
might identify.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>fig01 <span class="op">=</span> sns.scatterplot(X_train, x<span class="op">=</span>feature_names[<span class="dv">0</span>], y<span class="op">=</span>feature_names[<span class="dv">1</span>], hue<span class="op">=</span>dataset[<span class="st">'species'</span>])</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/e3_penguins_vis.png" alt="A scatter plot of the penguin dataset, showing bill length on the x-axis and bill depth on the y-axis. The points are coloured by species. There are three clusters of points, one for each species, with some overlap between the species." class="figure mx-auto d-block"><div class="figcaption">Visualising the penguins dataset</div>
</figure><p>As there are four measurements for each penguin, we need quite a few
plots to visualise all four dimensions against each other. Here is a
handy Seaborn function to do so:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>sns.pairplot(dataset, hue<span class="op">=</span><span class="st">"species"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/pairplot.png" alt="A pairplot of the penguin dataset, showing scatter plots of each pair of features. The points are coloured by species. There are three clusters of points, one for each species, with some overlap between the species." class="figure mx-auto d-block"><div class="figcaption">Visualising the penguins dataset</div>
</figure><p>We can see that penguins from each species form fairly distinct
spatial clusters in these plots, so that you could draw lines between
those clusters to delineate each species. This is effectively what many
classification algorithms do. They use the training data to delineate
the observation space, in this case the 4 measurement dimensions, into
classes. When given a new observation, the model finds which of those
class areas the new observation falls in to.</p>
</div>
</section><section><h2 class="section-heading" id="classification-using-a-decision-tree">Classification using a decision tree<a class="anchor" aria-label="anchor" href="#classification-using-a-decision-tree"></a>
</h2>
<hr class="half-width">
<p>We’ll first apply a decision tree classifier to the data. Decisions
trees are conceptually similar to flow diagrams (or more precisely for
the biologists: dichotomous keys). They split the classification problem
into a binary tree of comparisons, at each step comparing a measurement
to a value, and moving left or right down the tree until a
classification is reached.</p>
<figure><img src="fig/decision_tree_example.png" alt="A decision tree example showing how one might classify four species of animals based on their features. The tree starts with 'Has feathers?' and branches into 'Can Fly?' and 'Has fur?'. The outcomes are Hawk, Penguin, Bear, and Dolphin" class="figure mx-auto d-block"><div class="figcaption">Decision tree for classifying penguins</div>
</figure><p>Training and using a decision tree in Scikit-Learn is
straightforward:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, plot_tree</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>clf <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>clf.predict(X_test)</span></code></pre>
</div>
<div id="hyper-parameters-parameters-that-tune-a-model" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="hyper-parameters-parameters-that-tune-a-model" class="callout-inner">
<h3 class="callout-title">Hyper-parameters: parameters that tune a model</h3>
<div class="callout-content">
<p>‘Max Depth’ is an example of a <em>hyper-parameter</em> for the
decision tree model. Where models use the parameters of an observation
to predict a result, hyper-parameters are used to tune how a model
works. Each model you encounter will have its own set of
hyper-parameters, each of which affects model behaviour and performance
in a different way. The process of adjusting hyper-parameters in order
to improve model performance is called hyper-parameter tuning.</p>
</div>
</div>
</div>
<p>We can conveniently check how our model did with the .score()
function, which will make predictions and report what proportion of them
were accurate:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>clf_score <span class="op">=</span> clf.score(X_test, y_test)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="bu">print</span>(clf_score)</span></code></pre>
</div>
<p>Our model reports an accuracy of ~98% on the test data! We can also
look at the decision tree that was generated:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>plot_tree(clf, class_names<span class="op">=</span>class_names, feature_names<span class="op">=</span>feature_names, filled<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>fig.gca())</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/e3_dt_2.png" alt="An auomatically generated decision tree based on the model we just trained, showing how the model makes decisions based on the penguin features. The tree has 2 levels, with the first level splitting on flipper length, the second level splitting on bill length and bill depth." class="figure mx-auto d-block"><div class="figcaption">Decision tree for classifying penguins</div>
</figure><p>The first first question (<code>depth=1</code>) splits the training
data into “Adelie” and “Gentoo” categories using the criteria
<code>flipper_length_mm &lt;= 206.5</code>, and the next two questions
(<code>depth=2</code>) split the “Adelie” and “Gentoo” categories into
“Adelie &amp; Chinstrap” and “Gentoo &amp; Chinstrap” predictions.</p>
<!-- We can see from this that there's some very tortuous logic being used to tease out every single observation in the training set. For example, the single purple Gentoo node at the bottom of the tree. If we truncated that branch to the second level (Chinstrap), we'd have a little inaccuracy, a total of 9 non-Chinstraps in with 48 Chinstraps, but a less convoluted model.

The tortuous logic, such as the bottom purple Gentoo node, is a clear indication that this model has been over-fitted. It has developed a very complex delineation of the classification space in order to match every single observation, which will likely lead to poor results for new observations.

We can see that rather than clean lines between species, the decision tree produces orthogonal regions as each decision only considers a single parameter. Again, we can see that the model is over-fitting as the decision space is far more complex than needed, with regions that only select a single point. -->
<div class="section level3">
<h3 id="visualising-the-classification-space">Visualising the classification space<a class="anchor" aria-label="anchor" href="#visualising-the-classification-space"></a>
</h3>
<p>We can visualise the classification space (decision tree boundaries)
to get a more intuitive feel for what it is doing.Note that our 2D plot
can only show two parameters at a time, so we will quickly visualise by
training a new model on only 2 features:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> DecisionBoundaryDisplay</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>f1 <span class="op">=</span> feature_names[<span class="dv">0</span>]</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>f2 <span class="op">=</span> feature_names[<span class="dv">3</span>]</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>clf <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>clf.fit(X_train[[f1, f2]], y_train)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>d <span class="op">=</span> DecisionBoundaryDisplay.from_estimator(clf, X_train[[f1, f2]])</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>sns.scatterplot(X_train, x<span class="op">=</span>f1, y<span class="op">=</span>f2, hue<span class="op">=</span>y_train, palette<span class="op">=</span><span class="st">"husl"</span>)</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/e3_dt_space_2.png" alt="A scatter plot of the penguin daaset, showing bill length on the x-axis and body mass on the y-axis. The points are coloured by species. The decision tree is shown as colored regions, with the boundaries between the regions being orthogonal lines. The regions are generally aligned with the species clusters, but there are several misclassifications." class="figure mx-auto d-block"><div class="figcaption">Classification space for our decision tree</div>
</figure><div id="tuning-the-max_depth-hyperparameter" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="tuning-the-max_depth-hyperparameter" class="callout-inner">
<h3 class="callout-title">Tuning the <code>max_depth</code>
hyperparameter</h3>
<div class="callout-content">
<p>Our decision tree using a <code>max_depth=2</code> is fairly simple
and there are still some incorrect predictions in our final
classifications. What happens if we increase the <code>max_depth</code>
hyperparameter? Will our model improve?</p>
<p>Write a loop to train a decision tree classifier with
<code>max_depth</code> values of 1, 2, 3, 4 and 5, and record the
accuracy of each model on the test data. Then plot the accuracy against
the <code>max_depth</code> values.</p>
<p>Yout might find the following code snippets useful:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>clf <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>acc <span class="op">=</span> clf.score(X_test, y_test)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>sns.lineplot(acc_df, x<span class="op">=</span><span class="st">'depth'</span>, y<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>plt.xlabel(<span class="st">'Tree depth'</span>)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<p>What happens? Why?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>max_depths <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>accuracy <span class="op">=</span> []</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="cf">for</span> depth <span class="kw">in</span> max_depths:</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>    clf <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span>depth)</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>    clf.fit(X_train, y_train)</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>    acc <span class="op">=</span> clf.score(X_test, y_test)</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>    accuracy.append((depth, acc))</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>acc_df <span class="op">=</span> pd.DataFrame(accuracy, columns<span class="op">=</span>[<span class="st">'depth'</span>, <span class="st">'accuracy'</span>])</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>sns.lineplot(acc_df, x<span class="op">=</span><span class="st">'depth'</span>, y<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>plt.xlabel(<span class="st">'Tree depth'</span>)</span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/e3_dt_overfit.png" alt="A line plot showing the accuracy of decision trees with various max_depth hyper-parameters. The x-axis shows the max_depth, and the y-axis shows the accuracy. The accuracy is highest at max_depth=2." class="figure mx-auto d-block"><div class="figcaption">Performance of decision trees of various
depths</div>
</figure><p>It looks like a <code>max_depth=2</code> performs slightly better on
the test data than those with <code>max_depth &gt; 2</code>. This can
seem counter intuitive, as surely more questions should be able to
better split up our categories and thus give better predictions?</p>
<p>This is a classic case of over-fitting - our model has produced
extremely specific parameters that work for the training data but are
not necessarily representative of our test data.</p>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="inspecting-an-over-fitted-decision-tree">Inspecting an over-fitted decision tree<a class="anchor" aria-label="anchor" href="#inspecting-an-over-fitted-decision-tree"></a>
</h2>
<hr class="half-width">
<p>Let’s reuse our fitting and plotting codes from above to inspect a
decision tree that has <code>max_depth=5</code>:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>clf <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>plot_tree(clf, class_names<span class="op">=</span>class_names, feature_names<span class="op">=</span>feature_names, filled<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>fig.gca())</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/e3_dt_6.png" alt="An automatically generated decision tree based on the model we just trained, showing how the model makes decisions based on the penguin features. The tree has 4 levels, and is difficult to read." class="figure mx-auto d-block"><div class="figcaption">Simplified decision tree</div>
</figure><p>It looks like our decision tree has split up the training data into
the correct penguin categories and more accurately than the
<code>max_depth=2</code> model did, however it used some very specific
questions to split up the penguins into the correct categories. Let’s
try visualising the classification space for a more intuitive
understanding:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>f1 <span class="op">=</span> feature_names[<span class="dv">0</span>]</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>f2 <span class="op">=</span> feature_names[<span class="dv">3</span>]</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>clf <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>clf.fit(X_train[[f1, f2]], y_train)</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>d <span class="op">=</span> DecisionBoundaryDisplay.from_estimator(clf, X_train[[f1, f2]])</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>sns.scatterplot(X_train, x<span class="op">=</span>f1, y<span class="op">=</span>f2, hue<span class="op">=</span>y_train, palette<span class="op">=</span><span class="st">'husl'</span>)</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/e3_dt_space_6.png" alt="A scatter plot of the penguin daaset, showing bill length on the x-axis and body mass on the y-axis. The points are coloured by species. The decision tree is shown as colored regions, with the boundaries between the regions being orthogonal lines. The regions are generally aligned with the species clusters, but there are many small regions that only select a single point." class="figure mx-auto d-block"><div class="figcaption">Classification space of the simplified decision
tree</div>
</figure><p>Earlier we saw that the <code>max_depth=2</code> model split the data
into 3 simple bounding boxes, whereas for <code>max_depth=5</code> we
see the model has created some very specific classification boundaries
to correctly classify every point in the training data.</p>
<p>This is a classic case of over-fitting - our model has produced
extremely specific parameters that work for the training data but are
not representitive of our test data. Sometimes simplicity is better!</p>
</section><section><h2 class="section-heading" id="classification-using-support-vector-machines">Classification using support vector machines<a class="anchor" aria-label="anchor" href="#classification-using-support-vector-machines"></a>
</h2>
<hr class="half-width">
<p>Next, we’ll look at another commonly used classification algorithm,
and see how it compares. Support Vector Machines (SVM) work in a way
that is conceptually similar to your own intuition when first looking at
the data. They devise a set of hyperplanes that delineate the parameter
space, such that each region contains ideally only observations from one
class, and the boundaries fall between classes.</p>
<div class="section level3">
<h3 id="normalising-data">Normalising data<a class="anchor" aria-label="anchor" href="#normalising-data"></a>
</h3>
<p>Unlike decision trees, SVMs require an additional pre-processing step
for our data. We need to normalise it. Our raw data has parameters with
different magnitudes such as bill length measured in 10’s of mm’s,
whereas body mass is measured in 1000’s of grams. If we trained an SVM
directly on this data, it would only consider the parameter with the
greatest variance (body mass).</p>
<p>Normalising maps each parameter to a new range so that it has a mean
of 0 and a standard deviation of 1.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> preprocessing</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>scalar <span class="op">=</span> preprocessing.StandardScaler()</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>scalar.fit(X_train)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>X_train_scaled <span class="op">=</span> pd.DataFrame(scalar.transform(X_train), columns<span class="op">=</span>X_train.columns, index<span class="op">=</span>X_train.index)</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>X_test_scaled <span class="op">=</span> pd.DataFrame(scalar.transform(X_test), columns<span class="op">=</span>X_test.columns, index<span class="op">=</span>X_test.index)</span></code></pre>
</div>
<p>Note that we fit the scalar to our training data - we then use this
same pre-trained scalar to transform our testing data.</p>
<p>With this scaled data, training the models works exactly the same as
before.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>SVM <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">'poly'</span>, degree<span class="op">=</span><span class="dv">3</span>, C<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>SVM.fit(X_train_scaled, y_train)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>svm_score <span class="op">=</span> SVM.score(X_test_scaled, y_test)</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Decision tree score is "</span>, clf_score)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SVM score is "</span>, svm_score)</span></code></pre>
</div>
<p>We can again visualise the decision space produced, also using only
two parameters:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>x2 <span class="op">=</span> X_train_scaled[[feature_names[<span class="dv">0</span>], feature_names[<span class="dv">1</span>]]]</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>SVM <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">'poly'</span>, degree<span class="op">=</span><span class="dv">3</span>, C<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>SVM.fit(x2, y_train)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>DecisionBoundaryDisplay.from_estimator(SVM, x2) <span class="co">#, ax=ax</span></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>sns.scatterplot(x2, x<span class="op">=</span>feature_names[<span class="dv">0</span>], y<span class="op">=</span>feature_names[<span class="dv">1</span>], hue<span class="op">=</span>dataset[<span class="st">'species'</span>])</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/e3_svc_space.png" alt="A scatter plot of the penguin daaset, showing bill length on the x-axis and bill depth on the y-axis. The points are coloured by species. The SVM is shown as colored regions, with the boundaries between the regions being curved lines. The regions are generally aligned with the species clusters, but there are still several misclassifications." class="figure mx-auto d-block"><div class="figcaption">Classification space generated by the SVM
model</div>
</figure><p>While this SVM model performs slightly worse than our decision tree
(95.6% vs. 98.5%), it’s likely that the non-linear boundaries will
perform better when exposed to more and more real data, as decision
trees are prone to overfitting and requires complex linear models to
reproduce simple non-linear boundaries. It’s important to pick a model
that is appropriate for your problem and data trends!</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Classification requires labelled data (is supervised).</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-04-ensemble-methods"><p>Content from <a href="04-ensemble-methods.html">Ensemble methods</a></p>
<hr>
<p>Last updated on 2025-07-23 |

        <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/edit/main/episodes/04-ensemble-methods.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are ensemble methods?</li>
<li>What are random forests?</li>
<li>How can we stack estimators in sci-kit learn?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn about applying ensemble methods in scikit-learn.</li>
<li>Understand why ensemble methods are useful.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="ensemble-methods">Ensemble methods<a class="anchor" aria-label="anchor" href="#ensemble-methods"></a>
</h2>
<hr class="half-width">
<p>What’s better than one decision tree? Perhaps two? or three? How
about enough trees to make up a forest? Ensemble methods bundle
individual models together and use each of their outputs to contribute
towards a final consensus for a given problem. Ensemble methods are
based on the mantra that the whole is greater than the sum of the
parts.</p>
<p>Thinking back to the classification episode with decision trees we
quickly stumbled into the problem of overfitting our training data. If
we combine predictions from a series of over/under fitting estimators
then we can often produce a better final prediction than using a single
reliable model - in the same way that humans often hear multiple
opinions on a scenario before deciding a final outcome. Decision trees
and regressions are often very sensitive to training outliers and so are
well suited to be a part of an ensemble.</p>
<p>Ensemble methods are used for a variety of applciations including,
but not limited to, search systems and object detection. We can use any
model/estimator available in sci-kit learn to create an ensemble. There
are three main methods to create ensembles approaches:</p>
<ul>
<li>Stacking</li>
<li>Bagging</li>
<li>Boosting</li>
</ul>
<p>Let’s explore them in a bit more depth.</p>
<div class="section level3">
<h3 id="stacking">Stacking<a class="anchor" aria-label="anchor" href="#stacking"></a>
</h3>
<p>This is where we train a series of different models/estimators on the
same input data in parallel. We then take the output of each model and
pass them into a final decision algorithm/model that makes the final
prediction.</p>
<p>If we trained the same model multiple times on the same data we would
expect very similar answers, and so the emphasis with stacking is to
choose different models that can be used to build up a reliable
concensus. Regression is then typically a good choice for the final
decision-making model.</p>
<figure><img src="fig/stacking.jpeg" alt="A diagram showing how stacking works. It shows three different models being trained on the same data, and then their outputs being combined in a final model to make a prediction." class="figure mx-auto d-block"><div class="figcaption">Stacking</div>
</figure><p><a href="https://vas3k.com/blog/machine_learning/" class="external-link">Image from Vasily
Zubarev via their blog</a></p>
</div>
<div class="section level3">
<h3 id="bagging-a-k-a-bootstrap-aggregating">Bagging (a.k.a <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" class="external-link">Bootstrap
AGGregatING</a> )<a class="anchor" aria-label="anchor" href="#bagging-a-k-a-bootstrap-aggregating"></a>
</h3>
<p>This is where we use the same model/estimator and fit it on different
subsets of the training data. We can then average the results from each
model to produce a final prediction. The subsets are random and may even
repeat themselves.</p>
<p>The most common example is known as the Random Forest algorithm,
which we’ll take a look at later on. Random Forests are typically used
as a faster, computationally cheaper alternative to Neural Networks,
which is ideal for real-time applications like camera face detection
prompts.</p>
<figure><img src="fig/bagging.jpeg" alt="A diagram showing how bagging works. It shows the same model being trained on different subsets of the data, and then their outputs being averaged to make a prediction." class="figure mx-auto d-block"><div class="figcaption">Stacking</div>
</figure><p><a href="https://vas3k.com/blog/machine_learning/" class="external-link">Image from Vasily
Zubarev via their blog</a></p>
</div>
<div class="section level3">
<h3 id="boosting">Boosting<a class="anchor" aria-label="anchor" href="#boosting"></a>
</h3>
<p>This is where we train a single type of Model/estimator on an initial
dataset, test it’s accuracy, and then subsequently train the same type
of models on poorly predicted samples i.e. each new model pays most
attention to data that were incorrectly predicted by the last one.</p>
<p>Just like for bagging, boosting is trained mostly on subsets, however
in this case these subsets are not randomly generated but are instead
built using poorly estimated predictions. Boosting can produce some very
high accuracies by learning from it’s mistakes, but due to the iterative
nature of these improvements it doesn’t parallelize well unlike the
other ensemble methods. Despite this it can still be a faster, and
computationally cheaper alternative to Neural Networks.</p>
<figure><img src="fig/boosting.jpeg" alt="A diagram showing how boosting works. It shows the same model being trained on the same data, but with each iteration focusing on the samples that were poorly predicted by the previous iteration." class="figure mx-auto d-block"><div class="figcaption">Stacking</div>
</figure><p><a href="https://vas3k.com/blog/machine_learning/" class="external-link">Image from Vasily
Zubarev via their blog</a></p>
</div>
<div class="section level3">
<h3 id="ensemble-summary">Ensemble summary<a class="anchor" aria-label="anchor" href="#ensemble-summary"></a>
</h3>
<p>Machine learning jargon can often be hard to remember, so here is a
quick summary of the 3 ensemble methods:</p>
<ul>
<li>Stacking - same dataset, different models, trained in parallel</li>
<li>Bagging - different subsets, same models, trained in parallel</li>
<li>Boosting - subsets of bad estimates, same models, trained in
series</li>
</ul>
</div>
</section><section><h2 class="section-heading" id="using-bagging-random-forests-for-a-classification-problem">Using Bagging (Random Forests) for a classification problem<a class="anchor" aria-label="anchor" href="#using-bagging-random-forests-for-a-classification-problem"></a>
</h2>
<hr class="half-width">
<p>In this session we’ll take another look at the penguins data and
applying one of the most common bagging approaches, random forests, to
try and solve our species classification problem. First we’ll load in
the dataset and define a train and test split.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># import libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># load penguins data</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>penguins <span class="op">=</span> sns.load_dataset(<span class="st">'penguins'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># prepare and define our data and targets</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>feature_names <span class="op">=</span> [<span class="st">'bill_length_mm'</span>, <span class="st">'bill_depth_mm'</span>, <span class="st">'flipper_length_mm'</span>, <span class="st">'body_mass_g'</span>]</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>penguins.dropna(subset<span class="op">=</span>feature_names, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>species_names <span class="op">=</span> penguins[<span class="st">'species'</span>].unique()</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>X <span class="op">=</span> penguins[feature_names]</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>y <span class="op">=</span> penguins.species</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="co"># Split data in training and test set</span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"train size:"</span>, X_train.shape)</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"test size"</span>, X_test.shape)</span></code></pre>
</div>
<p>We’ll now take a look how we can use ensemble methods to perform a
classification task such as identifying penguin species! We’re going to
use a Random forest classifier available in scikit-learn which is a
widely used example of a bagging approach.</p>
<p>Random forests are built on decision trees and can provide another
way to address over-fitting. Rather than classifying based on one single
decision tree (which could overfit the data), an average of results of
many trees can be derived for more robust/accurate estimates compared
against single trees used in the ensemble.</p>
<figure><img src="fig/randomforest.png" alt="A diagram showing how a random forest works. It shows multiple decision trees being trained on different subsets of the data, and then their outputs being combined to make a prediction." class="figure mx-auto d-block"><div class="figcaption">Random Forests</div>
</figure><p><a href="https://commons.wikimedia.org/wiki/File:Random_forest_diagram_complete.png" class="external-link">Image
from Venkatak Jagannath</a></p>
<p>We can now define a random forest estimator and train it using the
penguin training data. We have a similar set of attritbutes to the
DecisionTreeClassifier but with an extra parameter called n_estimators
which is the number of trees in the forest.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> plot_tree</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># Define our model</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co"># extra parameter called n_estimators which is number of trees in the forest</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># a leaf is a class label at the end of the decision tree</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>forest <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, max_depth<span class="op">=</span><span class="dv">7</span>, min_samples_leaf<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="co"># train our model</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>forest.fit(X_train, y_train)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co"># Score our model</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="bu">print</span>(forest.score(X_test, y_test))</span></code></pre>
</div>
<p>You might notice that we have a different value (hopefully increased)
compared with the decision tree classifier used above on the same
training data. Lets plot the first 5 trees in the forest to get an idea
of how this model differs from a single decision tree.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">5</span> ,figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>))</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="co"># plot first 5 trees in forest</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="cf">for</span> index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">5</span>):</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    plot_tree(forest.estimators_[index],</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>        class_names<span class="op">=</span>species_names,</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>        feature_names<span class="op">=</span>feature_names,</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>        filled<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>        ax<span class="op">=</span>axes[index])</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>    axes[index].set_title(<span class="ss">f'Tree: </span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/rf_5_trees.png" alt="A figure showing the first 5 trees in a random forest model. Each tree is a decision tree with different splits based on the penguin features, and each tree has a different structure and depth." class="figure mx-auto d-block"><div class="figcaption">random forest trees</div>
</figure><p>We can see the first 5 (of 100) trees that were fitted as part of the
forest.</p>
<p>If we train the random forest estimator using the same two parameters
used to plot the classification space for the decision tree classifier
what do we think the plot will look like?</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># lets train a random forest for only two features (body mass and bill length)</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> DecisionBoundaryDisplay</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>f1 <span class="op">=</span> feature_names[<span class="dv">0</span>]</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>f2 <span class="op">=</span> feature_names[<span class="dv">3</span>]</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co"># plot classification space for body mass and bill length with random forest</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>forest_2d <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, max_depth<span class="op">=</span><span class="dv">7</span>, min_samples_leaf<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>forest_2d.fit(X_train[[f1, f2]], y_train)</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="co"># Lets plot the decision boundaries made by the model for the two trained features</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>d <span class="op">=</span> DecisionBoundaryDisplay.from_estimator(forest_2d, X_train[[f1, f2]])</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>sns.scatterplot(X_train, x<span class="op">=</span>f1, y<span class="op">=</span>f2, hue<span class="op">=</span>y_train, palette<span class="op">=</span><span class="st">"husl"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/EM_rf_clf_space.png" alt="A scatter plot of the penguin dataset, showing body mass on the x-axis and bill length on the y-axis. The points are coloured by species. The random forest classifier is shown as colored regions, with the boundaries between the regions being orthogonal lines. The regions are generally aligned with the species clusters, but there are still several misclassifications and a complicated decision space." class="figure mx-auto d-block"><div class="figcaption">random forest clf space</div>
</figure><p>There is still some overfitting indicated by the regions that contain
only single points but using the same hyper-parameter settings used to
fit the decision tree classifier, we can see that overfitting is
reduced.</p>
</section><section><h2 class="section-heading" id="stacking-a-regression-problem">Stacking a regression problem<a class="anchor" aria-label="anchor" href="#stacking-a-regression-problem"></a>
</h2>
<hr class="half-width">
<p>We’ve had a look at a bagging approach, but we’ll now take a look at
a stacking approach and apply it to a regression problem. We’ll also
introduce a new dataset to play around with.</p>
<div class="section level3">
<h3 id="california-house-price-prediction">California house price prediction<a class="anchor" aria-label="anchor" href="#california-house-price-prediction"></a>
</h3>
<p>The California housing dataset for regression problems contains 8
training features such as, Median Income, House Age, Average Rooms,
Average Bedrooms etc. for 20,640 properties. The target variable is the
median house value for those 20,640 properties, note that all prices are
in units of $100,000. This toy dataset is available as part of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html" class="external-link">scikit
learn library</a>. We’ll start by loading the dataset to very briefly
inspect the attributes by printing them out.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># load the dataset</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>X, y <span class="op">=</span> fetch_california_housing(return_X_y<span class="op">=</span><span class="va">True</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="co">## All price variables are in units of $100,000</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="bu">print</span>(X.shape)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="bu">print</span>(X.head())</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Housing price as the target: "</span>)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a><span class="co">## Target is in units of $100,000</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a><span class="bu">print</span>(y.head())</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a><span class="bu">print</span>(y.shape)</span></code></pre>
</div>
<p>For the the purposes of learning how to create and use ensemble
methods and since it is a toy dataset, we will blindly use this dataset
without inspecting it, cleaning or pre-processing it further.</p>
<div id="exercise-investigate-and-visualise-the-dataset" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="exercise-investigate-and-visualise-the-dataset" class="callout-inner">
<h3 class="callout-title">Exercise: Investigate and visualise the dataset</h3>
<div class="callout-content">
<p>For this episode we simply want to learn how to build and use an
Ensemble rather than actually solve a regression problem. To build up
your skills as an ML practitioner, investigate and visualise this
dataset. What can you say about the dataset itself, and what can you
summarise about about any potential relationships or prediction
problems?</p>
</div>
</div>
</div>
<p>Lets start by splitting the dataset into training and testing
subsets:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># split into train and test sets, We are selecting an 80%-20% train-test split.</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'train size: </span><span class="sc">{</span>X_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'test size: </span><span class="sc">{</span>X_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code></pre>
</div>
<p>Lets stack a series of regression models. In the same way the
RandomForest classifier derives a results from a series of trees, we
will combine the results from a series of different models in our stack.
This is done using what’s called an ensemble meta-estimator called a
VotingRegressor.</p>
<p>We’ll apply a Voting regressor to a random forest, gradient boosting
and linear regressor.</p>
<p>Lets stack a series of regression models. In the same way the
RandomForest classifier derives a results from a series of trees, we
will combine the results from a series of different models in our stack.
This is done using what’s called an ensemble meta-estimator called a
VotingRegressor.</p>
<p>We’ll apply a Voting regressor to a random forest, gradient boosting
and linear regressor.</p>
<div id="but-wait-arent-random-forestsdecision-tree-for-classification-problems" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="but-wait-arent-random-forestsdecision-tree-for-classification-problems" class="callout-inner">
<h3 class="callout-title">But wait, aren’t random forests/decision tree for classification problems?</h3>
<div class="callout-content">
<p>Yes they are, but quite often in machine learning various models can
be used to solve both regression and classification problems. Decision
trees in particular can be used to “predict” specific numerical values
instead of categories, essentially by binning a group of values into a
single value. This works well for periodic/repeating numerical data.
These trees are extremely sensitive to the data they are trained on,
which makes them a very good model to use as a Random Forest.</p>
</div>
</div>
</div>
<div id="but-wait-again-isnt-a-random-forest-and-a-gradient-boosting-model-an-ensemble-method-instead-of-a-regression-model" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="but-wait-again-isnt-a-random-forest-and-a-gradient-boosting-model-an-ensemble-method-instead-of-a-regression-model" class="callout-inner">
<h3 class="callout-title">But wait again, isn’t a random forest (and a gradient boosting model) an ensemble method instead of a regression
model?</h3>
<div class="callout-content">
<p>Yes they are, but they can be thought of as one big complex model
used like any other model. The awesome thing about ensemble methods, and
the generalisation of Scikit-Learn models, is that you can put an
ensemble in an ensemble!</p>
</div>
</div>
</div>
<p>A VotingRegressor can train several base estimators on the whole
dataset, and it can take the average of the individual predictions to
form a final prediction.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> (</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    GradientBoostingRegressor,</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    RandomForestRegressor,</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    VotingRegressor,</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co"># Initialize estimators</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>rf_reg <span class="op">=</span> RandomForestRegressor(random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>gb_reg <span class="op">=</span> GradientBoostingRegressor(random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>linear_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>voting_reg <span class="op">=</span> VotingRegressor([(<span class="st">"rf"</span>, rf_reg), (<span class="st">"gb"</span>, gb_reg), (<span class="st">"lr"</span>, linear_reg)])</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a><span class="co"># fit/train voting estimator</span></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>voting_reg.fit(X_train, y_train)</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a><span class="co"># lets also fit/train the individual models for comparison</span></span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>rf_reg.fit(X_train, y_train)</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>gb_reg.fit(X_train, y_train)</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>linear_reg.fit(X_train, y_train)</span></code></pre>
</div>
<p>We fit the voting regressor in the same way we would fit a single
model. When the voting regressor is instantiated we pass it a parameter
containing a list of tuples that contain the estimators we wish to
stack: in this case the random forest, gradient boosting and linear
regressors. To get a sense of what this is doing lets predict the first
20 samples in the test portion of the data and plot the results.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co"># make predictions</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>X_test_20 <span class="op">=</span> X_test[:<span class="dv">20</span>] <span class="co"># first 20 for visualisation</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>rf_pred <span class="op">=</span> rf_reg.predict(X_test_20)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>gb_pred <span class="op">=</span> gb_reg.predict(X_test_20)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>linear_pred <span class="op">=</span> linear_reg.predict(X_test_20)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>voting_pred <span class="op">=</span> voting_reg.predict(X_test_20)</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>plt.figure()</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>plt.plot(gb_pred,  <span class="st">"o"</span>, color<span class="op">=</span><span class="st">"black"</span>, label<span class="op">=</span><span class="st">"GradientBoostingRegressor"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>plt.plot(rf_pred,  <span class="st">"o"</span>, color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"RandomForestRegressor"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>plt.plot(linear_pred,  <span class="st">"o"</span>, color<span class="op">=</span><span class="st">"green"</span>, label<span class="op">=</span><span class="st">"LinearRegression"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>plt.plot(voting_pred,  <span class="st">"x"</span>, color<span class="op">=</span><span class="st">"red"</span>, ms<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">"VotingRegressor"</span>)</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>plt.tick_params(axis<span class="op">=</span><span class="st">"x"</span>, which<span class="op">=</span><span class="st">"both"</span>, bottom<span class="op">=</span><span class="va">False</span>, top<span class="op">=</span><span class="va">False</span>, labelbottom<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>plt.ylabel(<span class="st">"predicted"</span>)</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>plt.xlabel(<span class="st">"training samples"</span>)</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>plt.title(<span class="st">"Regressor predictions and their average"</span>)</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/house_price_voting_regressor.svg" alt="A plot showing the predictions of different regression models on the same dataset. Each model's predictions are represented by different colored markers, and the voting regressor's predictions are shown as larger red crosses." class="figure mx-auto d-block"><div class="figcaption">Regressor predictions and average from
stack</div>
</figure><p>Finally, lets see how the average compares against each single
estimator in the stack?</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'random forest: </span><span class="sc">{</span>rf_reg<span class="sc">.</span>score(X_test, y_test)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'gradient boost: </span><span class="sc">{</span>gb_reg<span class="sc">.</span>score(X_test, y_test)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'linear regression: </span><span class="sc">{</span>linear_reg<span class="sc">.</span>score(X_test, y_test)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'voting regressor: </span><span class="sc">{</span>voting_reg<span class="sc">.</span>score(X_test, y_test)<span class="sc">}</span><span class="ss">'</span>)</span></code></pre>
</div>
<p>Each of our models score between 0.61-0.82, which at the high end is
good, but at the low end is a pretty poor prediction accuracy score. Do
note that the toy datasets are not representative of real world data.
However what we can see is that the stacked result generated by the
voting regressor fits different sub-models and then averages the
individual predictions to form a final prediction. The benefit of this
approach is that, it reduces overfitting and increases generalizability.
Of course, we could try and improve our accuracy score by tweaking with
our indivdual model hyperparameters, using more advaced boosted models
or adjusting our training data features and train-test-split data.</p>
<div id="exercise-stacking-a-classification-problem." class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-stacking-a-classification-problem." class="callout-inner">
<h3 class="callout-title">Exercise: Stacking a classification problem.</h3>
<div class="callout-content">
<p>Scikit learn also has method for stacking ensemble classifiers
<code>sklearn.ensemble.VotingClassifier</code> do you think you could
apply a stack to the penguins dataset using a random forest, SVM and
decision tree classifier, or a selection of any other classifier
estimators available in sci-kit learn?</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>penguins <span class="op">=</span> sns.load_dataset(<span class="st">'penguins'</span>)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>feature_names <span class="op">=</span> [<span class="st">'bill_length_mm'</span>, <span class="st">'bill_depth_mm'</span>, <span class="st">'flipper_length_mm'</span>, <span class="st">'body_mass_g'</span>]</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>penguins.dropna(subset<span class="op">=</span>feature_names, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>species_names <span class="op">=</span> penguins[<span class="st">'species'</span>].unique()</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a><span class="co"># Define data and targets</span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>X <span class="op">=</span> penguins[feature_names]</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>y <span class="op">=</span> penguins.species</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a><span class="co"># Split data in training and test set</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'train size: </span><span class="sc">{</span>X_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'test size: </span><span class="sc">{</span>X_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code></pre>
</div>
<p>The code above loads the penguins data and splits it into test and
training portions. Have a play around with stacking some classifiers
using the <code>sklearn.ensemble.VotingClassifier</code> using the code
comments below as a guide.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="co"># import classifiers</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="co"># instantiate classifiers</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="co"># fit classifiers</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="co"># instantiate voting classifier and fit data</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a><span class="co"># make predictions</span></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a><span class="co"># compare scores</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">

</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Ensemble methods can be used to reduce under/over fitting training
data.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-05-clustering"><p>Content from <a href="05-clustering.html">Unsupervised methods - Clustering</a></p>
<hr>
<p>Last updated on 2025-07-23 |

        <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/edit/main/episodes/05-clustering.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is unsupervised learning?</li>
<li>How can we use clustering to find data points with similar
attributes?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the difference between supervised and unsupervised
learning.</li>
<li>Identify clusters in data using k-means clustering.</li>
<li>Understand the limitations of k-means when clusters overlap.</li>
<li>Use spectral clustering to overcome the limitations of k-means.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="unsupervised-learning">Unsupervised learning<a class="anchor" aria-label="anchor" href="#unsupervised-learning"></a>
</h2>
<hr class="half-width">
<p>In episode 2 we learnt about supervised learning. Now it is time to
explore unsupervised learning.</p>
<p>Sometimes we do not have the luxury of using labelled data. This
could be for a number of reasons:</p>
<ul>
<li>We have labelled data, but not enough to accurately train our
model</li>
<li>Our existing labelled data is low-quality or innacurate</li>
<li>It is too time-consuming to (manually) label more data</li>
<li>We have data, but no idea what correlations might exist that we
could model!</li>
</ul>
<p>In this case we need to use unsupervised learning. As the name
suggests, this time we do not “supervise” the ML algorithm by providing
it labels, but instead we let it try to find its own patterns in the
data and report back on any correlations that it might find. You can
think of unsupervised learning as a way to discover labels from the data
itself.</p>
</section><section><h2 class="section-heading" id="clustering">Clustering<a class="anchor" aria-label="anchor" href="#clustering"></a>
</h2>
<hr class="half-width">
<p>Clustering is the grouping of data points which are similar to each
other. It can be a powerful technique for identifying patterns in data.
Clustering analysis does not usually require any training and is
therefore known as an unsupervised learning technique. Clustering can be
applied quickly due to this lack of training.</p>
</section><section><h2 class="section-heading" id="applications-of-clustering">Applications of clustering<a class="anchor" aria-label="anchor" href="#applications-of-clustering"></a>
</h2>
<hr class="half-width">
<ul>
<li>Looking for trends in data</li>
<li>Reducing the data around a point to just that point (e.g. reducing
colour depth in an image)</li>
<li>Pattern recognition</li>
</ul></section><section><h2 class="section-heading" id="k-means-clustering">K-means clustering<a class="anchor" aria-label="anchor" href="#k-means-clustering"></a>
</h2>
<hr class="half-width">
<p>The k-means clustering algorithm is a simple clustering algorithm
that tries to identify the centre of each cluster. It does this by
searching for a point which minimises the distance between the centre
and all the points in the cluster. The algorithm needs to be told how
many k clusters to look for, but a common technique is to try different
numbers of clusters and combine it with other tests to decide on the
best combination.</p>
<div id="hyper-parameters-again" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="hyper-parameters-again" class="callout-inner">
<h3 class="callout-title">Hyper-parameters again</h3>
<div class="callout-content">
<p>‘K’ is also an exmaple of a <em>hyper-parameter</em> for the k-means
clustering technique. Another example of a hyper-parameter is the
N-degrees of freedom for polynomial regression. Keep an eye out for
others throughout the lesson!</p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="k-means-with-scikit-learn">K-means with Scikit-Learn<a class="anchor" aria-label="anchor" href="#k-means-with-scikit-learn"></a>
</h3>
<p>To perform a k-means clustering with Scikit-Learn we first need to
import the sklearn.cluster module.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> sklearn.cluster <span class="im">as</span> skl_cluster</span></code></pre>
</div>
<p>For this example, we’re going to use Scikit-Learn’s built-in ‘random
data blob generator’ instead of using an external dataset. Therefore
we’ll need the <code>sklearn.datasets.samples_generator</code>
module.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> sklearn.datasets <span class="im">as</span> skl_datasets</span></code></pre>
</div>
<p>Now lets create some random blobs using the <code>make_blobs</code>
function. The <code>n_samples</code> argument sets how many points we
want to use in all of our blobs while <code>cluster_std</code> sets the
standard deviation of the points. The smaller this value the closer
together they will be. <code>centers</code> sets how many clusters we’d
like. <code>random_state</code> is the initial state of the random
number generator. By specifying this value we’ll get the same results
every time we run the program. If we don’t specify a random state then
we’ll get different points every time we run. This function returns two
things: an array of data points and a list of which cluster each point
belongs to.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co">#Lets define some functions here to avoid repetitive code</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="kw">def</span> plots_labels(data, labels):</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    tx <span class="op">=</span> data[:, <span class="dv">0</span>]</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    ty <span class="op">=</span> data[:, <span class="dv">1</span>]</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    plt.scatter(tx, ty, edgecolor<span class="op">=</span><span class="st">'k'</span>, c<span class="op">=</span>labels)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    plt.show()</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a><span class="kw">def</span> plot_clusters(data, clusters, Kmean):</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>    tx <span class="op">=</span> data[:, <span class="dv">0</span>]</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>    ty <span class="op">=</span> data[:, <span class="dv">1</span>]</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>    plt.scatter(tx, ty, s<span class="op">=</span><span class="dv">5</span>, linewidth<span class="op">=</span><span class="dv">0</span>, c<span class="op">=</span>clusters)</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>    <span class="cf">for</span> cluster_x, cluster_y <span class="kw">in</span> Kmean.cluster_centers_:</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>        plt.scatter(cluster_x, cluster_y, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'r'</span>, marker<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>    plt.show()</span></code></pre>
</div>
<p>Lets create the clusters.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>data, cluster_id <span class="op">=</span> skl_datasets.make_blobs(n_samples<span class="op">=</span><span class="dv">400</span>, cluster_std<span class="op">=</span><span class="fl">0.75</span>, centers<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>plots_labels(data, cluster_id)</span></code></pre>
</div>
<figure><img src="fig/random_clusters.png" alt="A scatter plot of randomly generated clusters. The points are coloured by their cluster id, with four distinct clusters visible." class="figure mx-auto d-block"><div class="figcaption">Plot of the random clusters</div>
</figure><p>Now that we have some data we can try to identify the clusters using
k-means. First, we need to initialise the KMeans module and tell it how
many clusters to look for. Next, we supply it with some data via the
<code>fit</code> function, in much the same way we did with the
regression functions earlier on. Finally, we run the predict function to
find the clusters.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>Kmean <span class="op">=</span> skl_cluster.KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>Kmean.fit(data)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>clusters <span class="op">=</span> Kmean.predict(data)</span></code></pre>
</div>
<p>The data can now be plotted to show all the points we randomly
generated. To make it clearer which cluster points have been classified
we can set the colours (the c parameter) to use the
<code>clusters</code> list that was returned by the <code>predict</code>
function. The Kmeans algorithm also lets us know where it identified the
centre of each cluster. These are stored as a list called
‘cluster_centers_’ inside the <code>Kmean</code> object. Let’s plot the
points from the clusters, colouring them by the output from the K-means
algorithm, and also plot the centres of each cluster as a red X.</p>
<pre><code><span><span class="fu">plot_clusters</span><span class="op">(</span><span class="va">data</span>, <span class="va">clusters</span>, <span class="va">Kmean</span><span class="op">)</span></span></code></pre>
<figure><img src="fig/random_clusters_centre.png" alt="A scatter plot of the random clusters, with the points coloured by their cluster id. The centres of each cluster are marked with a red X." class="figure mx-auto d-block"><div class="figcaption">Plot of the fitted random clusters</div>
</figure><p>Here is the code all in a single block.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">import</span> sklearn.cluster <span class="im">as</span> skl_cluster</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="im">import</span> sklearn.datasets <span class="im">as</span> skl_datasets</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>data, cluster_id <span class="op">=</span> skl_datasets.make_blobs(n_samples<span class="op">=</span><span class="dv">400</span>, cluster_std<span class="op">=</span><span class="fl">0.75</span>, centers<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>Kmean <span class="op">=</span> skl_cluster.KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>Kmean.fit(data)</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>clusters <span class="op">=</span> Kmean.predict(data)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>plot_clusters(data, clusters, Kmean)</span></code></pre>
</div>
<div id="working-in-multiple-dimensions" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="working-in-multiple-dimensions" class="callout-inner">
<h3 class="callout-title">Working in multiple dimensions</h3>
<div class="callout-content">
<p>Although this example shows two dimensions, the kmeans algorithm can
work in more than two. It becomes very difficult to show this visually
once we get beyond 3 dimensions. Its very common in machine learning to
be working with multiple variables and so our classifiers are working in
multi-dimensional spaces.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="limitations-of-k-means">Limitations of k-means<a class="anchor" aria-label="anchor" href="#limitations-of-k-means"></a>
</h3>
<ul>
<li>Requires number of clusters to be known in advance</li>
<li>Struggles when clusters have irregular shapes</li>
<li>Will always produce an answer finding the required number of
clusters even if the data isn’t clustered (or clustered in that many
clusters)</li>
<li>Requires linear cluster boundaries</li>
</ul>
<figure><img src="fig/kmeans_concentric_circle.png" alt="A scatter plot showing the failure of k-means clustering on non-linear cluster boundaries. The points are coloured by their cluster id, with two distinct circular clusters visible. Each circle of points is half in one cluster and half in the other." class="figure mx-auto d-block"><div class="figcaption">An example of kmeans failing on non-linear
cluster boundaries</div>
</figure>
</div>
<div class="section level3">
<h3 id="advantages-of-k-means">Advantages of k-means<a class="anchor" aria-label="anchor" href="#advantages-of-k-means"></a>
</h3>
<ul>
<li>Simple algorithm and fast to compute</li>
<li>A good choice as the first thing to try when attempting to cluster
data</li>
<li>Suitable for large datasets due to its low memory and computing
requirements</li>
</ul>
<div id="exercise-k-means-with-overlapping-clusters" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-k-means-with-overlapping-clusters" class="callout-inner">
<h3 class="callout-title">Exercise: K-Means with overlapping clusters</h3>
<div class="callout-content">
<p>Adjust the program above to increase the standard deviation of the
blobs (the cluster_std parameter to make_blobs) and increase the number
of samples (n_samples) to 4000. You should start to see the clusters
overlapping. Do the clusters that are identified make sense? Is there
any strange behaviour?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Increasing n_samples to 4000 and cluster_std to 3.0 looks like
this:</p>
<figure><img src="fig/kmeans_overlapping_clusters.png" alt="A scatter plot showing two rings of points." class="figure mx-auto d-block"><div class="figcaption">Kmeans attempting to classify overlapping
clusters</div>
</figure><p>The straight line boundaries between clusters look a bit strange.</p>
</div>
</div>
</div>
</div>
<div id="exercise-how-many-clusters-should-we-look-for" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-how-many-clusters-should-we-look-for" class="callout-inner">
<h3 class="callout-title">Exercise: How many clusters should we look for?</h3>
<div class="callout-content">
<p>Using k-means requires us to specify the number of clusters to
expect. A common strategy to get around this is to vary the number of
clusters we are looking for. Modify the program to loop through
searching for between 2 and 10 clusters. Which (if any) of the results
look more sensible? What criteria might you use to select the best
one?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="cf">for</span> cluster_count <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">11</span>):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    Kmean <span class="op">=</span> skl_cluster.KMeans(n_clusters<span class="op">=</span>cluster_count)</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    Kmean.fit(data)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>    clusters <span class="op">=</span> Kmean.predict(data)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>    plt.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">5</span>, linewidth<span class="op">=</span><span class="dv">0</span>,c<span class="op">=</span>clusters)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    <span class="cf">for</span> cluster_x, cluster_y <span class="kw">in</span> Kmean.cluster_centers_:</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>        plt.scatter(cluster_x, cluster_y, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'r'</span>, marker<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>        <span class="co"># give the graph a title with the number of clusters</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>        plt.title(<span class="bu">str</span>(cluster_count)<span class="op">+</span><span class="st">" Clusters"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    plt.show()</span></code></pre>
</div>
<p>None of these look like very sensible clusterings because all of the
points form one large cluster. We might look at a measure of similarity
to test if this single cluster is actually multiple clusters. A simple
standard deviation or interquartile range might be a good starting
point.</p>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="spectral-clustering">Spectral clustering<a class="anchor" aria-label="anchor" href="#spectral-clustering"></a>
</h2>
<hr class="half-width">
<p>Spectral clustering is a technique that attempts to overcome the
linear boundary problem of k-means clustering. It works by treating
clustering as a graph partitioning problem and looks for nodes in a
graph with a small distance between them. See <a href="https://www.cvl.isy.liu.se/education/graduate/spectral-clustering/SC_course_part1.pdf" class="external-link">this
introduction to spectral clustering</a> if you are interested in more
details about how spectral clustering works.</p>
<p>Here is an example of spectral clustering on two concentric
circles:</p>
<figure><img src="fig/spectral_concentric_circle.png" alt="A scatter plot showing the results of spectral clustering on two concentric circles. The points are coloured by their cluster id, with one circle in red and the other in black. Unlike k-means, the clusters are not split by a straight line." class="figure mx-auto d-block"><div class="figcaption">Spectral clustering on two concentric
circles</div>
</figure><p>Spectral clustering uses something called a ‘kernel trick’ to
introduce additional dimensions to the data. A common example of this is
trying to cluster one circle within another (concentric circles). A
k-means classifier will fail to do this and will end up effectively
drawing a line which crosses the circles. However spectral clustering
will introduce an additional dimension that effectively moves one of the
circles away from the other in the additional dimension. This does have
the downside of being more computationally expensive than k-means
clustering.</p>
<figure><img src="fig/spectral_concentric_3d.png" alt="A 3D scatter plot showing the results of spectral clustering on two concentric circles. The points are coloured by their cluster id, with one circle in red and the other in black. The circles are separated vertically in the third dimension." class="figure mx-auto d-block"><div class="figcaption">Spectral clustering viewed with an extra
dimension</div>
</figure><div class="section level3">
<h3 id="spectral-clustering-with-scikit-learn">Spectral clustering with Scikit-Learn<a class="anchor" aria-label="anchor" href="#spectral-clustering-with-scikit-learn"></a>
</h3>
<p>Lets try out using Scikit-Learn’s spectral clustering. To make the
concentric circles in the above example we need to use the
<code>make_circles</code> function in the sklearn.datasets module. This
works in a very similar way to the make_blobs function we used earlier
on.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">import</span> sklearn.datasets <span class="im">as</span> skl_data</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>circles, circles_clusters <span class="op">=</span> skl_data.make_circles(n_samples<span class="op">=</span><span class="dv">400</span>, noise<span class="op">=</span><span class="fl">.01</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>plots_labels(circles, circles_clusters)</span></code></pre>
</div>
<p>The code for calculating the SpectralClustering is very similar to
the kmeans clustering, but instead of using the sklearn.cluster.KMeans
class we use the <code>sklearn.cluster.SpectralClustering</code>
class.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>model <span class="op">=</span> skl_cluster.SpectralClustering(n_clusters<span class="op">=</span><span class="dv">2</span>, affinity<span class="op">=</span><span class="st">'nearest_neighbors'</span>, assign_labels<span class="op">=</span><span class="st">'kmeans'</span>)</span></code></pre>
</div>
<p>The SpectralClustering class combines the fit and predict functions
into a single function called fit_predict.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>labels <span class="op">=</span> model.fit_predict(circles)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>plots_labels(circles, labels)</span></code></pre>
</div>
<p>Here is the whole program combined with the kmeans clustering for
comparison. Note that this produces two figures. To view both of them
use the “Inline” graphics terminal inside the Python console instead of
the “Automatic” method which will open a window and only show you one of
the graphs.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">import</span> sklearn.cluster <span class="im">as</span> skl_cluster</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="im">import</span> sklearn.datasets <span class="im">as</span> skl_data</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>circles, circles_clusters <span class="op">=</span> skl_data.make_circles(n_samples<span class="op">=</span><span class="dv">400</span>, noise<span class="op">=</span><span class="fl">.01</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="co"># cluster with kmeans</span></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>Kmean <span class="op">=</span> skl_cluster.KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>Kmean.fit(circles)</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>clusters <span class="op">=</span> Kmean.predict(circles)</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a><span class="co"># plot the data, colouring it by cluster</span></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>plot_clusters(circles, clusters, Kmean)</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a><span class="co"># cluster with spectral clustering</span></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>model <span class="op">=</span> skl_cluster.SpectralClustering(n_clusters<span class="op">=</span><span class="dv">2</span>, affinity<span class="op">=</span><span class="st">'nearest_neighbors'</span>, assign_labels<span class="op">=</span><span class="st">'kmeans'</span>)</span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>labels <span class="op">=</span> model.fit_predict(circles)</span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a>plots_labels(circles, labels)</span></code></pre>
</div>
<figure><img src="fig/kmeans_concentric_circle_2.png" alt="A scatter plot showing the results of k-means clustering on two concentric circles. The points are coloured by their cluster id, with one circle in yellow and the other in purple. The clusters are split by a straight line." class="figure mx-auto d-block"><div class="figcaption">Kmeans attempting to cluster the concentric
circles</div>
</figure><figure><img src="fig/spectral_concentric_circle_2.png" alt="A scatter plot showing the results of spectral clustering on two concentric circles. The points are coloured by their cluster id, with one circle in yellow and the other in purple. Unlike k-means, the clusters are not split by a straight line and are correctly identified." class="figure mx-auto d-block"><div class="figcaption">Spectral clustering on the concentric
circles</div>
</figure><div id="comparing-k-means-and-spectral-clustering-performance" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="comparing-k-means-and-spectral-clustering-performance" class="callout-inner">
<h3 class="callout-title">Comparing k-means and spectral clustering performance</h3>
<div class="callout-content">
<p>Modify the program we wrote in the previous exercise to use spectral
clustering instead of k-means and save it as a new file.</p>
<p>Time how long both programs take to run. Add the line
<code>import time</code> at the top of both files as the first line, and
get the start time with <code>start_time = time.time()</code>.</p>
<p>End the program by getting the time again and subtracting the start
time from it to get the total run time. Add
<code>end_time = time.time()</code> and
<code>print("Elapsed time:",end_time-start_time,"seconds")</code> to the
end of both files.</p>
<p>Compare how long both programs take to run generating 4,000 samples
and testing them for between 2 and 10 clusters. - How much did your run
times differ? - How much do they differ if you increase the number of
samples to 8,000? - How long do you think it would take to compute
800,000 samples (estimate this, it might take a while to run for
real)?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>KMeans version: runtime around 4 seconds (your computer might be
faster/slower)</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="im">import</span> sklearn.cluster <span class="im">as</span> skl_cluster</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>data, cluster_id <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">4000</span>, cluster_std<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>                                       centers<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a><span class="cf">for</span> cluster_count <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">11</span>):</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>    Kmean <span class="op">=</span> skl_cluster.KMeans(n_clusters<span class="op">=</span>cluster_count)</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>    Kmean.fit(data)</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>    clusters <span class="op">=</span> Kmean.predict(data)</span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>    plt.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">15</span>, linewidth<span class="op">=</span><span class="dv">0</span>, c<span class="op">=</span>clusters)</span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>    plt.title(<span class="bu">str</span>(cluster_count)<span class="op">+</span><span class="st">" Clusters"</span>)</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a>plt.show()</span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Elapsed time = "</span>, end_time<span class="op">-</span>start_time, <span class="st">"seconds"</span>)</span></code></pre>
</div>
<p>Spectral version: runtime around 9 seconds (your computer might be
faster/slower)</p>
<pre><code>import matplotlib.pyplot as plt
import sklearn.cluster as skl_cluster
from sklearn.datasets import make_blobs
import time

start_time = time.time()
data, cluster_id = make_blobs(n_samples=4000, cluster_std=3,
                                       centers=4, random_state=1)

for cluster_count in range(2,11):
    model = skl_cluster.SpectralClustering(n_clusters=cluster_count,
                                       affinity='nearest_neighbors',
                                       assign_labels='kmeans')
    labels = model.fit_predict(data)

    plt.scatter(data[:, 0], data[:, 1], s=15, linewidth=0, c=labels)
    plt.title(str(cluster_count)+" Clusters")
plt.show()
end_time = time.time()
print("Elapsed time = ", end_time-start_time, "seconds")</code></pre>
<p>When the number of points increases to 8000 the runtimes are 24
seconds for the spectral version and 5.6 seconds for kmeans.</p>
<p>The runtime numbers will differ depending on the speed of your
computer, but the relative difference should be similar.</p>
<p>For 4000 points kmeans took 4 seconds, while spectral took 9 seconds.
A 2.25 fold difference.</p>
<p>For 8000 points kmeans took 5.6 seconds, while spectral took 24
seconds. A 4.28 fold difference. Kmeans is 1.4 times slower for double
the data, while spectral is 2.6 times slower.</p>
<p>The relative difference is diverging. If we used 100 times more data
we might expect a 100 fold divergence in execution times.</p>
<p>Kmeans might take a few minutes while spectral will take hours.</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Clustering is a form of unsupervised learning.</li>
<li>Unsupervised learning algorithms don’t need training.</li>
<li>Kmeans is a popular clustering algorithm.</li>
<li>Kmeans is less useful when one cluster exists within another, such
as concentric circles.</li>
<li>Spectral clustering can overcome some of the limitations of
Kmeans.</li>
<li>Spectral clustering is much slower than Kmeans.</li>
<li>Scikit-Learn has functions to create example data.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-06-dimensionality-reduction"><p>Content from <a href="06-dimensionality-reduction.html">Unsupervised methods - Dimensionality reduction</a></p>
<hr>
<p>Last updated on 2025-07-23 |

        <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/edit/main/episodes/06-dimensionality-reduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do we apply machine learning techniques to data with higher
dimensions?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Recall that most data is inherently multidimensional.</li>
<li>Understand that reducing the number of dimensions can simplify
modelling and allow classifications to be performed.</li>
<li>Apply Principle Component Analysis (PCA) and t-distributed
Stochastic Neighbor Embedding (t-SNE) to reduce the dimensions of
data.</li>
<li>Evaluate the relative performance of PCA and t-SNE in reducing data
dimensionality.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="dimensionality-reduction">Dimensionality reduction<a class="anchor" aria-label="anchor" href="#dimensionality-reduction"></a>
</h2>
<hr class="half-width">
<p>As seen in the last episode, general clustering algorithms work well
with low-dimensional data. In this episode we see how higher-dimensional
data, such as images of handwritten text or numbers, can be processed
with dimensionality reduction techniques to make the datasets more
accessible for other modelling techniques. The dataset we will be using
is the Scikit-Learn subset of the Modified National Institute of
Standards and Technology (MNIST) dataset.</p>
<figure><img src="fig/MnistExamples.png" alt="A grid of images showing examples of handwritten digits from 0 to 9. Each image is a greyscale image of a single digit, with the digits varying in size and style." class="figure mx-auto d-block"><div class="figcaption">MNIST example illustrating all the classes in
the dataset</div>
</figure><p>The MNIST dataset contains 70,000 images of handwritten numbers, and
are labelled from 0-9 with the number that each image contains. Each
image is a greyscale and 28x28 pixels in size for a total of 784 pixels
per image. Each pixel can take a value between 0-255 (8bits). When
dealing with a series of images in machine learning we consider each
pixel to be a feature that varies according to each of the sample
images. Our previous penguin dataset only had no more than 7 features to
train with, however even a small 28x28 MNIST image has as much as 784
features (pixels) to work with.</p>
<figure><img src="fig/mnist_30000-letter.png" alt="A zoomed in greyscale image of a handwritten number '3'." class="figure mx-auto d-block"><div class="figcaption">MNIST example of a single image</div>
</figure><p>To make this episode a bit less computationally intensive, the
Scikit-Learn example that we will work with is a smaller sample of 1797
images. Each image is 8x8 in size for a total of 64 pixels per image,
resulting in 64 features for us to work with. The pixels can take a
value between 0-15 (4bits). Let’s retrieve and inspect the Scikit-Learn
dataset with the following code:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> sklearn.cluster <span class="im">as</span> skl_cluster</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> manifold, decomposition, datasets</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># Let's define these here to avoid repetitive code</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="kw">def</span> plots_labels(data, labels):</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>    tx <span class="op">=</span> data[:, <span class="dv">0</span>]</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>    ty <span class="op">=</span> data[:, <span class="dv">1</span>]</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>    plt.scatter(tx, ty, edgecolor<span class="op">=</span><span class="st">'k'</span>, c<span class="op">=</span>labels)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>    plt.show()</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="kw">def</span> plot_clusters(data, clusters, Kmean):</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>    tx <span class="op">=</span> data[:, <span class="dv">0</span>]</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>    ty <span class="op">=</span> data[:, <span class="dv">1</span>]</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>    plt.scatter(tx, ty, s<span class="op">=</span><span class="dv">5</span>, linewidth<span class="op">=</span><span class="dv">0</span>, c<span class="op">=</span>clusters)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>    <span class="cf">for</span> cluster_x, cluster_y <span class="kw">in</span> Kmean.cluster_centers_:</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>        plt.scatter(cluster_x, cluster_y, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'r'</span>, marker<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>    plt.show()</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a><span class="kw">def</span> plot_clusters_labels(data, labels):</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>    tx <span class="op">=</span> data[:, <span class="dv">0</span>]</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>    ty <span class="op">=</span> data[:, <span class="dv">1</span>]</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a>    <span class="co"># with labels</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">4</span>))</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a>    plt.scatter(tx, ty, c<span class="op">=</span>labels, cmap<span class="op">=</span><span class="st">"nipy_spectral"</span>,</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a>            edgecolor<span class="op">=</span><span class="st">'k'</span>, label<span class="op">=</span>labels)</span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a>    plt.colorbar(boundaries<span class="op">=</span>np.arange(<span class="dv">11</span>)<span class="op">-</span><span class="fl">0.5</span>).set_ticks(np.arange(<span class="dv">10</span>))</span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>    plt.show()</span></code></pre>
</div>
<p>Next lets load in the digits dataset,</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># load in dataset as a Pandas Dataframe, return X and Y</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>features, labels <span class="op">=</span> datasets.load_digits(return_X_y<span class="op">=</span><span class="va">True</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="bu">print</span>(features.shape, labels.shape)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="bu">print</span>(labels)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>features.head()</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="our-goal-using-dimensionality-reduction-to-help-with-machine-learning">Our goal: using dimensionality-reduction to help with machine
learning<a class="anchor" aria-label="anchor" href="#our-goal-using-dimensionality-reduction-to-help-with-machine-learning"></a>
</h2>
<hr class="half-width">
<p>As humans we are pretty good at object and pattern recognition. We
can look at the images above, inspect the intensity and position pixels
relative to other pixels, and pretty quickly make an accurate guess at
what the image shows. As humans we spends much of our younger lives
learning these spatial relations, and so it stands to reason that
computers can also extract these relations. Let’s see if it is possible
to use unsupervised clustering techniques to pull out relations in our
MNIST dataset of number images.</p>
<div id="exercise-try-to-visually-inspect-the-dataset-and-features-for-correlations" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-try-to-visually-inspect-the-dataset-and-features-for-correlations" class="callout-inner">
<h3 class="callout-title">Exercise: Try to visually inspect the dataset and features for correlations</h3>
<div class="callout-content">
<p>As we did for previous datasets, lets visually inspect relationships
between our features/pixels. Try and investigate the following pixels
for relations (written “row_column”): 0_4, 1_4, 2_4, and 3_4.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="bu">print</span>(features.iloc[<span class="dv">0</span>])</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>image_1D <span class="op">=</span> features.iloc[<span class="dv">0</span>]</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>image_2D <span class="op">=</span> np.array(image_1D).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">8</span>)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>plt.imshow(image_2D,cmap<span class="op">=</span><span class="st">"gray_r"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co"># these points are the pixels we will investigate</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="co"># pixels 0,1,2,3 of row 4 of the image</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>plt.plot([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],[<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>],<span class="st">"rx"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/mnist_pairplot_pixels.png" alt="A pixelated image of a handwritten '0', with red crosses marking the pixels at positions 0_4, 1_4, 2_4, and 3_4." class="figure mx-auto d-block"><div class="figcaption">SKLearn image with highlighted pixels</div>
</figure><pre><code>import seaborn as sns

# make a temporary copy of data for plotting here only
seaborn_data = features

# add labels for pairplot color coding
seaborn_data["labels"] = labels

# make a short list of N features for plotting N*N figures
# 4**2 = 16 plots, whereas 64**2 is over 4000!
feature_subset = []
for i in range(4):
    feature_subset.append("pixel_"+str(i)+"_4")

sns.pairplot(seaborn_data, vars=feature_subset, hue="labels",
             palette=sns.mpl_palette("Spectral", n_colors=10))</code></pre>
<figure><img src="fig/mnist_pairplot.png" alt="A pairplot of the MNIST dataset, showing the relationships between the pixels at positions 0_4, 1_4, 2_4, and 3_4. Each plot is coloured by the digit label, with distinct clusters visible for some digits." class="figure mx-auto d-block"><div class="figcaption">SKLearn image with highlighted pixels</div>
</figure><p>As we can see the dataset relations are far more complex than our
previous examples. The histograms show that some numbers appear in those
pixel positions more than others, but the
<code>feature_vs_feature</code> plots are quite messy to try and
decipher. There are gaps and patches of colour suggesting that there is
some kind of structure there, but it’s far harder to inspect than the
penguin data. We can’t easily see definitive clusters in our 2D
representations, and we know our clustering algorithms will take a long
time to try and crunch 64 dimensions at once, so let’s see if we can
represent our 64D data in fewer dimensions.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="dimensionality-reduction-with-scikit-learn">Dimensionality reduction with Scikit-Learn<a class="anchor" aria-label="anchor" href="#dimensionality-reduction-with-scikit-learn"></a>
</h2>
<hr class="half-width">
<p>We will look at two commonly used techniques for dimensionality
reduction: Principal Component Analysis (PCA) and t-distributed
Stochastic Neighbor Embedding (t-SNE). Both of these techniques are
supported by Scikit-Learn.</p>
<div class="section level3">
<h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)<a class="anchor" aria-label="anchor" href="#principal-component-analysis-pca"></a>
</h3>
<p>PCA allows us to replace our 64 features with a smaller number of
dimensional representations that retain the majority of our
variance/relational data. Using Scikit-Learn lets apply PCA in a
relatively simple way.</p>
<p>For more in depth explanations of PCA please see the following links:
* <a href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis" class="external-link">https://builtin.com/data-science/step-step-explanation-principal-component-analysis</a>
* <a href="https://scikit-learn.org/stable/modules/decomposition.html#pca" class="external-link">https://scikit-learn.org/stable/modules/decomposition.html#pca</a></p>
<p>Let’s apply PCA to the MNIST dataset and retain the two most-major
components:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># PCA with 2 components</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>pca <span class="op">=</span> decomposition.PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>x_pca <span class="op">=</span> pca.fit_transform(features)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(x_pca.shape)</span></code></pre>
</div>
<p>This returns us an array of 1797x2 where the 2 remaining columns(our
new “features” or “dimensions”) contain vector representations of the
first principle components (column 0) and second principle components
(column 1) for each of the images. We can plot these two new features
against each other:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># We are passing None becuase it is an unlabelled plot</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>plots_labels(x_pca, <span class="va">None</span>)</span></code></pre>
</div>
<figure><img src="fig/pca_unlabelled.png" alt="A scatter plot of the PCA reduced data. The points are not coloured by label, and so the clusters are not clearly visible." class="figure mx-auto d-block"><div class="figcaption">Reduction using PCA</div>
</figure><p>We now have a 2D representation of our 64D dataset that we can work
with instead. Let’s try some quick K-means clustering on our 2D
representation of the data. Because we already have some knowledge about
our data we can set <code>k=10</code> for the 10 digits present in the
dataset.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>Kmean <span class="op">=</span> skl_cluster.KMeans(n_clusters<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>Kmean.fit(x_pca)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>clusters <span class="op">=</span> Kmean.predict(x_pca)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>plot_clusters(x_pca, clusters, Kmean)</span></code></pre>
</div>
<figure><img src="fig/pca_clustered.png" alt="A scatter plot of the PCA reduced data, with the points coloured by their cluster id. The centres of each cluster are marked with a red X." class="figure mx-auto d-block"><div class="figcaption">Reduction using PCA</div>
</figure><p>And now we can compare how these clusters look against our actual
image labels by colour coding our first scatter plot:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>plot_clusters_labels(x_pca, labels)</span></code></pre>
</div>
<figure><img src="fig/pca_labelled.png" alt="A scatter plot of the PCA reduced data, with the points coloured by their digit label. The clusters are not clearly visible, with some digits overlapping significantly." class="figure mx-auto d-block"><div class="figcaption">Reduction using PCA</div>
</figure><p>PCA has done a valiant effort to reduce the dimensionality of our
problem from 64D to 2D while still retaining some of our key structural
information. We can see that the digits
<code>0</code>,<code>1</code>,<code>4</code>, and <code>6</code> cluster
up reasonably well even using a simple k-means test. However it does
look like there is still quite a bit of overlap between the remaining
digits, especially for the digits <code>5</code> and <code>8</code>. The
clustering is from perfect in the largest “blob”, but not a bad effort
from PCA given the substantial dimensionality reduction.</p>
<p>It’s worth noting that PCA does not handle outlier data well
primarily due to global preservation of structural information, and so
we will now look at a more complex form of learning that we can apply to
this problem.</p>
</div>
<div class="section level3">
<h3 id="t-distributed-stochastic-neighbor-embedding-t-sne">t-distributed Stochastic Neighbor Embedding (t-SNE)<a class="anchor" aria-label="anchor" href="#t-distributed-stochastic-neighbor-embedding-t-sne"></a>
</h3>
<p>t-SNE is a powerful example of manifold learning - a
non-deterministic non-linear approach to dimensionality reduction.
Manifold learning tasks are based on the idea that the dimension of many
datasets is artificially high. This is likely the case for our MNIST
dataset, as the corner pixels of our images are unlikely to contain
digit data, and thus those dimensions are almost negligable compared
with others.</p>
<p>The versatility of the algorithm in transforming the underlying
structural information into lower-order projections makes t-SNE
applicable to a wide range of research domains.</p>
<p>For more in depth explanations of t-SNE and manifold learning please
see the following links which also contain som very nice visual examples
of manifold learning in action: * <a href="https://thedatafrog.com/en/articles/visualizing-datasets/" class="external-link">https://thedatafrog.com/en/articles/visualizing-datasets/</a>
* <a href="https://scikit-learn.org/stable/modules/manifold.html" class="external-link">https://scikit-learn.org/stable/modules/manifold.html</a></p>
<p>Scikit-Learn allows us to apply t-SNE in a relatively simple way.
Lets code and apply t-SNE to the MNIST dataset in the same manner that
we did for the PCA example, and reduce the data down from 64D to 2D
again:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># t-SNE embedding</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="co"># initialising with "pca" explicitly preserves global structure</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>tsne <span class="op">=</span> manifold.TSNE(n_components<span class="op">=</span><span class="dv">2</span>, init<span class="op">=</span><span class="st">'pca'</span>, random_state <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>x_tsne <span class="op">=</span> tsne.fit_transform(features)</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>plots_labels(x_tsne, <span class="va">None</span>)</span></code></pre>
</div>
<figure><img src="fig/tsne_unlabelled.png" alt="A scatter plot of the t-SNE reduced data. The points are not yet coloured by label, but already several claer clusters are visible." class="figure mx-auto d-block"><div class="figcaption">Reduction using PCA</div>
</figure><p>It looks like t-SNE has done a much better job of splitting our data
up into clusters using only a 2D representation of the data. Once again,
let’s run a simple k-means clustering on this new 2D representation, and
compare with the actual color-labelled data:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>Kmean <span class="op">=</span> skl_cluster.KMeans(n_clusters<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>Kmean.fit(x_tsne)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>clusters <span class="op">=</span> Kmean.predict(x_tsne)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>plot_clusters(x_tsne, clusters, Kmean)</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>plot_clusters_labels(x_tsne, labels)</span></code></pre>
</div>
<figure><img src="fig/tsne_clustered.png" alt="A scatter plot of the t-SNE reduced data, with the points coloured by their cluster id. The centres of each cluster are marked with a red X." class="figure mx-auto d-block"><div class="figcaption">Reduction using PCA</div>
</figure><figure><img src="fig/tsne_labelled.png" alt="A scatter plot of the t-SNE reduced data, with the points coloured by their digit label. The clusters are clearly visible." class="figure mx-auto d-block"><div class="figcaption">Reduction using PCA</div>
</figure><p>It looks like t-SNE has successfully separated out our digits into
accurate clusters using as little as a 2D representation and a simple
k-means clustering algorithm. It has worked so well that you can clearly
see several clusters which can be modelled, whereas for our PCA
representation we needed to rely heavily on the knowledge that we had 10
types of digits to cluster.</p>
<p>Additionally, if we had run k-means on all 64 dimensions this would
likely still be computing away, whereas we have already broken down our
dataset into accurate clusters, with only a handful of outliers and
potential misidentifications (remember, a good ML model isn’t a perfect
model!)</p>
<p>The major drawback of applying t-SNE to datasets is the large
computational requirement. Furthermore, hyper-parameter tuning of t-SNE
usually requires some trial and error to perfect.</p>
<p>Our example here is still a relatively simple example of 8x8 images
and not very typical of the modern problems that can now be solved in
the field of ML and DL. To account for even higher-order input data,
neural networks were developed to more accurately extract feature
information.</p>
<div id="exercise-working-in-three-dimensions" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-working-in-three-dimensions" class="callout-inner">
<h3 class="callout-title">Exercise: Working in three dimensions</h3>
<div class="callout-content">
<p>The above example has considered only two dimensions since humans can
visualize two dimensions very well. However, there can be cases where a
dataset requires more than two dimensions to be appropriately
decomposed. Modify the above programs to use three dimensions and create
appropriate plots. Do three dimensions allow one to better distinguish
between the digits?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="co"># PCA</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>pca <span class="op">=</span> decomposition.PCA(n_components<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>pca.fit(features)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>x_pca <span class="op">=</span> pca.transform(features)</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>ax.scatter(x_pca[:, <span class="dv">0</span>], x_pca[:, <span class="dv">1</span>], x_pca[:, <span class="dv">2</span>], c<span class="op">=</span>labels,</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>          cmap<span class="op">=</span>plt.cm.nipy_spectral, s<span class="op">=</span><span class="dv">9</span>, lw<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/pca_3d.svg" alt="A 3D scatter plot showing the results of PCA on the MNIST dataset. The points are coloured by their digit label, with distinct clusters visible for some digits." class="figure mx-auto d-block"><div class="figcaption">Reduction to 3 components using pca</div>
</figure><div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># t-SNE embedding</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>tsne <span class="op">=</span> manifold.TSNE(n_components<span class="op">=</span><span class="dv">3</span>, init<span class="op">=</span><span class="st">'pca'</span>,</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>        random_state <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>x_tsne <span class="op">=</span> tsne.fit_transform(features)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>ax.scatter(x_tsne[:, <span class="dv">0</span>], x_tsne[:, <span class="dv">1</span>], x_tsne[:, <span class="dv">2</span>], c<span class="op">=</span>labels,</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>          cmap<span class="op">=</span>plt.cm.nipy_spectral, s<span class="op">=</span><span class="dv">9</span>, lw<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/tsne_3d.svg" alt="A 3D scatter plot showing the results of t-SNE on the MNIST dataset. The points are coloured by their digit label, with distinct clusters visible for some digits." class="figure mx-auto d-block"><div class="figcaption">Reduction to 3 components using tsne</div>
</figure>
</div>
</div>
</div>
</div>
<div id="exercise-parameters" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-parameters" class="callout-inner">
<h3 class="callout-title">Exercise: Parameters</h3>
<div class="callout-content">
<p>Look up parameters that can be changed in PCA and t-SNE, and
experiment with these. How do they change your resulting plots? Might
the choice of parameters lead you to make different conclusions about
your data?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">

</div>
</div>
</div>
</div>
<div id="exercise-other-algorithms" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-other-algorithms" class="callout-inner">
<h3 class="callout-title">Exercise: Other algorithms</h3>
<div class="callout-content">
<p>There are other algorithms that can be used for doing dimensionality
reduction (for example the Higher Order Singular Value Decomposition
(HOSVD)). Do an internet search for some of these and examine the
example data that they are used on. Are there cases where they do
poorly? What level of care might you need to use before applying such
methods for automation in critical scenarios? What about for interactive
data exploration?</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">

</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>PCA is a linear dimensionality reduction technique for tabular
data.</li>
<li>t-SNE is another dimensionality reduction technique for tabular data
that is more general than PCA.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-07-neural-networks"><p>Content from <a href="07-neural-networks.html">Neural Networks</a></p>
<hr>
<p>Last updated on 2025-07-23 |

        <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/edit/main/episodes/07-neural-networks.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are Neural Networks?</li>
<li>How can we classify images using a neural network?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the basic architecture of a perceptron.</li>
<li>Be able to create a perceptron to encode a simple function.</li>
<li>Understand that layers of perceptrons allow non-linear separable
problems to be solved.</li>
<li>Train a multi-layer perceptron using Scikit-Learn.</li>
<li>Evaluate the accuracy of a multi-layer perceptron using real input
data.</li>
<li>Understand that cross validation allows the entire data set to be
used in the training process.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="neural-networks">Neural networks<a class="anchor" aria-label="anchor" href="#neural-networks"></a>
</h2>
<hr class="half-width">
<p>Neural networks are a machine learning method inspired by how the
human brain works. They are particularly good at pattern recognition and
classification tasks, often using images as inputs. They are a
well-established machine learning technique, having been around since
the 1950s, but they’ve gone through several iterations to overcome
limitations in previous generations. Using state-of-the-art neural
networks is often referred to as ‘deep learning’.</p>
</section><section><h2 class="section-heading" id="perceptrons">Perceptrons<a class="anchor" aria-label="anchor" href="#perceptrons"></a>
</h2>
<hr class="half-width">
<p>Perceptrons are the building blocks of neural networks. They are an
artificial version of a single neuron in the brain. They typically have
one or more inputs and a single output. Each input will be multiplied by
a weight and the value of all the weighted inputs are then summed
together. Finally, the summed value is put through an activation
function which decides if the neuron “fires” a signal. In some cases,
this activation function is simply a threshold step function which
outputs zero below a certain input and one above it. Other designs of
neurons use other activation functions, but typically they have an
output between zero and one and are still step-like in their nature.</p>
<figure><img src="fig/perceptron.svg" alt="A diagram of a perceptron, showing three inputs, leading to a summation unit, then a thresholding unit, and finally an output." class="figure mx-auto d-block"><div class="figcaption">A diagram of a perceptron</div>
</figure><div class="section level3">
<h3 id="coding-a-perceptron">Coding a perceptron<a class="anchor" aria-label="anchor" href="#coding-a-perceptron"></a>
</h3>
<p>Below is an example of a perceptron written as a Python function. The
function takes three parameters: <code>Inputs</code> is a list of input
values, <code>Weights</code> is a list of weight values and
<code>Threshold</code> is the activation threshold.</p>
<p>First we multiply each input by the corresponding weight. To do this
quickly and concisely, we will use the numpy multiply function which can
multiply each item in a list by a corresponding item in another
list.</p>
<p>We then take the sum of all the inputs multiplied by their weights.
Finally, if this value is less than the activation threshold, we output
zero, otherwise we output a one.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="kw">def</span> perceptron(inputs, weights, threshold):</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(inputs) <span class="op">==</span> <span class="bu">len</span>(weights)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>    <span class="co"># multiply the inputs and weights</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>    values <span class="op">=</span> np.multiply(inputs,weights)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>    <span class="co"># sum the results</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>    total <span class="op">=</span> <span class="bu">sum</span>(values)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>    <span class="co"># decide if we should activate the perceptron</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>    <span class="cf">if</span> total <span class="op">&lt;</span> threshold:</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="computing-with-a-perceptron">Computing with a perceptron<a class="anchor" aria-label="anchor" href="#computing-with-a-perceptron"></a>
</h3>
<p>A single perceptron can perform basic linear classification problems
such as computing the logical AND, OR, and NOT functions.</p>
<p>OR</p>
<table class="table">
<thead><tr class="header">
<th>Input 1</th>
<th>Input 2</th>
<th>Output</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>AND</p>
<table class="table">
<thead><tr class="header">
<th>Input 1</th>
<th>Input 2</th>
<th>Output</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>NOT</p>
<table class="table">
<thead><tr class="header">
<th>Input 1</th>
<th>Output</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>We can get a single perceptron to compute each of these functions</p>
<p>OR:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>inputs <span class="op">=</span> [[<span class="fl">0.0</span>,<span class="fl">0.0</span>],[<span class="fl">1.0</span>,<span class="fl">0.0</span>],[<span class="fl">0.0</span>,<span class="fl">1.0</span>],[<span class="fl">1.0</span>,<span class="fl">1.0</span>]]</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="cf">for</span> <span class="bu">input</span> <span class="kw">in</span> inputs:</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">input</span>,perceptron(<span class="bu">input</span>, [<span class="fl">0.5</span>,<span class="fl">0.5</span>], <span class="fl">0.5</span>))</span></code></pre>
</div>
<p>AND:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>inputs <span class="op">=</span> [[<span class="fl">0.0</span>,<span class="fl">0.0</span>],[<span class="fl">1.0</span>,<span class="fl">0.0</span>],[<span class="fl">0.0</span>,<span class="fl">1.0</span>],[<span class="fl">1.0</span>,<span class="fl">1.0</span>]]</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="cf">for</span> <span class="bu">input</span> <span class="kw">in</span> inputs:</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">input</span>,perceptron(<span class="bu">input</span>, [<span class="fl">0.5</span>,<span class="fl">0.5</span>], <span class="fl">1.0</span>))</span></code></pre>
</div>
<p>NOT:</p>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>The NOT function only has a single input. To make it work in the
perceptron we need to introduce a bias term which is always the same
value. In this example it is the second input. It has a weight of 1.0
while the weight on the real input is -1.0.</p>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>inputs <span class="op">=</span> [[<span class="fl">0.0</span>,<span class="fl">1.0</span>],[<span class="fl">1.0</span>,<span class="fl">1.0</span>]]</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="cf">for</span> <span class="bu">input</span> <span class="kw">in</span> inputs:</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">input</span>,perceptron(<span class="bu">input</span>, [<span class="op">-</span><span class="fl">1.0</span>,<span class="fl">1.0</span>], <span class="fl">1.0</span>))</span></code></pre>
</div>
<p>A perceptron can be trained to compute any function which has linear
separability. A simple training algorithm called the perceptron learning
algorithm can be used to do this and Scikit-Learn has its own
implementation of it. We are going to skip over the perceptron learning
algorithm and move straight onto more powerful techniques. If you want
to learn more about it see <a href="https://computing.dcu.ie/~humphrys/Notes/Neural/single.neural.html" class="external-link">this
page</a> from Dublin City University.</p>
</div>
<div class="section level3">
<h3 id="perceptron-limitations">Perceptron limitations<a class="anchor" aria-label="anchor" href="#perceptron-limitations"></a>
</h3>
<p>A single perceptron cannot solve any function that is not linearly
separable, meaning that we need to be able to divide the classes of
inputs and outputs with a straight line. A common example of this is the
XOR function shown below:</p>
<table class="table">
<thead><tr class="header">
<th>Input 1</th>
<th>Input 2</th>
<th>Output</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>(Make a graph of this)</p>
<p>This function outputs a zero when all its inputs are one or zero and
its not possible to separate with a straight line. This is known as
linear separability. When this limitation was discovered in the 1960s it
effectively halted development of neural networks for over a decade in a
period known as the “AI Winter”.</p>
</div>
</section><section><h2 class="section-heading" id="multi-layer-perceptrons">Multi-layer perceptrons<a class="anchor" aria-label="anchor" href="#multi-layer-perceptrons"></a>
</h2>
<hr class="half-width">
<p>A single perceptron cannot be used to solve a non-linearly separable
function. For that, we need to use multiple perceptrons and typically
multiple layers of perceptrons. They are formed of networks of
artificial neurons which each take one or more inputs and typically have
a single output. The neurons are connected together in networks of 10s
to 1000s of neurons. Typically, networks are connected in layers with an
input layer, middle or hidden layer (or layers), and finally an output
layer.</p>
<figure><img src="fig/multilayer_perceptron.svg" alt="A diagram of a multi-layer perceptron, showing an input layer with 3 inputs, a hidden layer with 2 neurons, and an output layer with 3 outputs. The connections between the layers are shown." class="figure mx-auto d-block"><div class="figcaption">A multi-layer perceptron</div>
</figure><div class="section level3">
<h3 id="training-multi-layer-perceptrons">Training multi-layer perceptrons<a class="anchor" aria-label="anchor" href="#training-multi-layer-perceptrons"></a>
</h3>
<p>Multi-layer perceptrons need to be trained by showing them a set of
training data and measuring the error between the network’s predicted
output and the true value. Training takes an iterative approach that
improves the network a little each time a new training example is
presented. There are a number of training algorithms available for a
neural network today, but we are going to use one of the best
established and well known, the backpropagation algorithm. This
algorithm is called back propagation because it takes the error
calculated between an output of the network and the true value and takes
it back through the network to update the weights.</p>
<!--If you want to read more about back propagation, please see [this chapter](http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf) from the book "Neural Networks - A Systematic Introduction". THIS LINK IS BROKEN. -->
</div>
<div class="section level3">
<h3 id="multi-layer-perceptrons-in-scikit-learn">Multi-layer perceptrons in Scikit-Learn<a class="anchor" aria-label="anchor" href="#multi-layer-perceptrons-in-scikit-learn"></a>
</h3>
<p>We are going to build a multi-layer perceptron for recognising
handwriting from images. Scikit-Learn includes some example handwriting
data from the <a href="https://github.com/cvdfoundation/mnist" class="external-link">MNIST
data set</a>, which is a dataset containing 70,000 images of
hand-written digits. Each image is 28x28 pixels in size (784 pixels in
total) and is represented in grayscale with values between zero for
fully black and 255 for fully white. This means we will need 784
perceptrons in our input layer, each taking the input of one pixel and
10 perceptrons in our output layer to represent each digit we might
classify. If trained correctly, only the perceptron in the output layer
will “fire” to represent the contents of the image (but this is a
massive oversimplification!).</p>
<p>We can import this dataset from <code>sklearn.datasets</code> then
load it into memory by calling the <code>fetch_openml</code>
function.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> sklearn.datasets <span class="im">as</span> skl_data</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>data, labels <span class="op">=</span> skl_data.fetch_openml(<span class="st">'mnist_784'</span>, version<span class="op">=</span><span class="dv">1</span>, return_X_y<span class="op">=</span><span class="va">True</span>)</span></code></pre>
</div>
<p>This creates two arrays of data, one called <code>data</code> which
contains the image data and the other <code>labels</code> that contains
the labels for those images which will tell us which digit is in the
image. A common convention is to call the data <code>X</code> and the
labels <code>y</code>.</p>
<p>As neural networks typically want to work with data that ranges
between 0 and 1.0 we need to normalise our data to this range. Python
has a shortcut which lets us divide the entire data array by 255 and
store the result, we can simply do:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>data <span class="op">=</span> data <span class="op">/</span> <span class="fl">255.0</span></span></code></pre>
</div>
<p>This is instead of writing a loop ourselves to divide every pixel by
255. Although the final result is the same and will take about the same
amount of computation (possibly a little less, it might do some clever
optimisations).</p>
<p>Now we need to initialise a neural network. Scikit-Learn has an
entire library for this (<code>sklearn.neural_network</code>) and the
<code>MLPClassifier</code> class handles multi-layer perceptrons. This
network takes a few parameters including the size of the hidden layer,
the maximum number of training iterations we’re going to allow, the
exact algorithm to use, whether or not we’d like verbose output about
what the training is doing, and the initial state of the random number
generator.</p>
<p>In this example we specify a multi-layer perceptron with 50 hidden
nodes, we allow a maximum of 50 iterations to train it, we turn on
verbose output to see what’s happening, and initialise the random state
to 1 so that we always get the same behaviour.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">import</span> sklearn.neural_network <span class="im">as</span> skl_nn</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>mlp <span class="op">=</span> skl_nn.MLPClassifier(hidden_layer_sizes<span class="op">=</span>(<span class="dv">50</span>,), max_iter<span class="op">=</span><span class="dv">50</span>, verbose<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span></code></pre>
</div>
<p>We now have a neural network but we have not trained it yet. Before
training, we will split our dataset into two parts: a training set which
we will use to train the classifier and a test set which we will use to
see how well the training is working. By using different data for the
two, we can avoid ‘over-fitting’, which is the creation of models which
do not “generalise” or work with data other than their training
data.</p>
<p>Typically, the majority of the data will be used as training data
(70-90%), to help avoid overfitting. Let us see how big our dataset is
to decide how many samples we want to train with.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>data.shape</span></code></pre>
</div>
<p>This tells us we have 70,000 rows in the dataset. Let us take 90% of
the data for training and 10% for testing, so we will use the first
63,000 samples in the dataset as the training data and the last 7,000 as
the test data.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="co"># Assuming `data` is your feature matrix and `labels` is your target vector</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    data.values,        <span class="co"># Features</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    labels.values,      <span class="co"># Labels</span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.1</span>,      <span class="co"># Reserve 10% of data for testing</span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>     <span class="co"># For reproducibility</span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>)</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>X_train.shape</span></code></pre>
</div>
<p>Now lets train the network. This line will take about one minute to
run. We do this by calling the <code>fit</code> function inside the
<code>mlp</code> class instance. This needs two arguments: the data
itself, and the labels showing what class each item should be classified
to.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>mlp.fit(X_train, y_train)</span></code></pre>
</div>
<p>Finally, we will score the accuracy of our network against both the
original training data and the test data. If the training had converged
to the point where each iteration of training was not improving the
accuracy, then the accuracy of the training data should be 1.0
(100%).</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training set score"</span>, mlp.score(X_train, y_train))</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Testing set score"</span>, mlp.score(X_test, y_test))</span></code></pre>
</div>
<p>Here is the complete program:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="im">import</span> sklearn.datasets <span class="im">as</span> skl_data</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="im">import</span> sklearn.neural_network <span class="im">as</span> skl_nn</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>data, labels <span class="op">=</span> skl_data.fetch_openml(<span class="st">'mnist_784'</span>, version<span class="op">=</span><span class="dv">1</span>, return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>data <span class="op">=</span> data <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>mlp <span class="op">=</span> skl_nn.MLPClassifier(hidden_layer_sizes<span class="op">=</span>(<span class="dv">50</span>,), max_iter<span class="op">=</span><span class="dv">50</span>, verbose<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a>    data.values,        <span class="co"># Features</span></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>    labels.values,      <span class="co"># Labels</span></span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.1</span>,      <span class="co"># Reserve 10% of data for testing</span></span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>     <span class="co"># For reproducibility</span></span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>)</span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a>mlp.fit(X_train, y_train)</span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training set score"</span>, mlp.score(X_train, y_train))</span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Testing set score"</span>, mlp.score(X_test, y_test))</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="prediction-using-a-multi-layer-perceptron">Prediction using a multi-layer perceptron<a class="anchor" aria-label="anchor" href="#prediction-using-a-multi-layer-perceptron"></a>
</h3>
<p>Now that we have trained a multi-layer perceptron, we can give it
some input data and ask it to perform a prediction. In this case, our
input data is a 28x28 pixel image, which can also be represented as a
784-element list of data. The output will be a number between 0 and 9
telling us which digit the network thinks we have supplied. The
<code>predict</code> function in the <code>MLPClassifier</code> class
can be used to make a prediction. Lets use the first digit from our test
set as an example.</p>
<p>Before we can pass it to the predictor, we need to extract one of the
digits from the test set. We can use <code>iloc</code> on the dataframe
to get hold of the first element in the test set. In order to present it
to the predictor, we have to turn it into a numpy array which has the
dimensions of 1x784 instead of 28x28. We can then call the
<code>predict</code> function with this array as our parameter. This
will return an array of predictions (as it could have been given
multiple inputs), the first element of this will be the predicted digit.
You may get a warning stating “X does not have valid feature names”,
this is because we didn’t encode feature names into our X (digit images)
data.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>test_digit <span class="op">=</span> X_test[<span class="dv">0</span>].reshape(<span class="dv">1</span>,<span class="dv">784</span>) <span class="co"># current shape is (784,)</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>test_digit_prediction <span class="op">=</span> mlp.predict(test_digit)[<span class="dv">0</span>]</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted value"</span>,test_digit_prediction)</span></code></pre>
</div>
<p>We can now verify if the prediction is correct by looking at the
corresponding item in the <code>labels_test</code> array.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Actual value"</span>,y_test[<span class="dv">0</span>])</span></code></pre>
</div>
<p>This should be the same value which is being predicted.</p>
<div id="changing-the-learning-parameters" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="changing-the-learning-parameters" class="callout-inner">
<h3 class="callout-title">Changing the learning parameters</h3>
<div class="callout-content">
<p>There are several parameters which control the training of the data.
One of these is called the learning rate. Increasing this can reduce how
many learning iterations we need. But if this is too large you can end
up overshooting. Try tweaking this parameter by adding the parameter
<code>learning_rate_init</code> with a default value of 0.001. Try
increasing it to around 0.1.</p>
</div>
</div>
</div>
<div id="using-your-own-handwriting" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="using-your-own-handwriting" class="callout-inner">
<h3 class="callout-title">Using your own handwriting</h3>
<div class="callout-content">
<p>Create an image using Microsoft Paint, the GNU Image Manipulation
Project (GIMP) or <a href="https://jspaint.app/" class="external-link">jspaint</a>. The image
needs to be grayscale and 28 x 28 pixels.</p>
<p>Try to draw a digit (0-9) in the image and save it into your code
directory.</p>
<p>The code below loads the image (called digit.png, change to whatever
your file is called) using the OpenCV library. Some Anaconda
installations need this installed either through the package manager or
by running the command: <code>conda install -c conda-forge opencv</code>
from the anaconda terminal.</p>
<p>OpenCV assumes that images are 3 channel red, green, blue and we have
to convert to one channel grayscale with <code>cvtColor</code>.</p>
<p>We also need to normalise the image by dividing each pixel by
255.</p>
<p>To verify the image, we can plot it by using OpenCV’s
<code>imshow</code> function (we could also use Matplotlib’s
<code>matshow</code> function).</p>
<p>To check what digit it is, we can pass it into
<code>mlp.predict</code>, but we have to convert it from a 28x28 array
to a one dimensional 784-byte long array with the <code>reshape</code>
function.</p>
<p>Did it correctly classify your hand(mouse) writing? Try a few images.
If you have time try drawing images on a touch screen or taking a photo
of something you have really written by hand. Remember that you will
have to resize it to be 28x28 pixels.</p>
<pre><code>import cv2
import matplotlib.pyplot as plt
digit = cv2.imread("digit.png")
digit_gray = cv2.cvtColor(digit, cv2.COLOR_BGR2GRAY)
digit_norm = digit_gray/255.0
cv2.imshow("Normalised Digit",digit_norm)
print("Your digit is",mlp.predict(digit_norm.reshape(1,784)))</code></pre>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="measuring-neural-network-performance">Measuring neural network performance<a class="anchor" aria-label="anchor" href="#measuring-neural-network-performance"></a>
</h2>
<hr class="half-width">
<p>We have now trained a neural network and tested prediction on a few
images. This might have given us a feel for how well our network is
performing, but it would be much more useful to have a more objective
measure. Since recognising digits is a classification problem, we can
measure how many predictions were correct in a set of test data. As we
already have a test set of data with 7,000 images we can use that and
see how many predictions the neural network has gotten right. We will
loop through every image in the test set, run it through our predictor
and compare the result with the label for that image. We will also keep
a tally of how many images we got right and see what percentage were
correct.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>correct<span class="op">=</span><span class="dv">0</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="cf">for</span> idx, row <span class="kw">in</span> <span class="bu">enumerate</span>(data_test):</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    <span class="co"># image contains a tuple of the row number and image data</span></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>    image <span class="op">=</span> row.reshape(<span class="dv">1</span>,<span class="dv">784</span>)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>    prediction <span class="op">=</span> mlp.predict(image)[<span class="dv">0</span>]</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>    actual <span class="op">=</span> labels_test[idx]</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a>    <span class="cf">if</span> prediction <span class="op">==</span> actual:</span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a>        correct <span class="op">=</span> correct <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a><span class="bu">print</span>((correct<span class="op">/</span><span class="bu">len</span>(data_test))<span class="op">*</span><span class="dv">100</span>)</span></code></pre>
</div>
<div class="section level3">
<h3 id="confusion-matrix">Confusion matrix<a class="anchor" aria-label="anchor" href="#confusion-matrix"></a>
</h3>
<p>We now know what percentage of images were correctly classified, but
we don’t know anything about the distribution of correct predictions
across our different classes (the digits 0 to 9 in this case). A more
powerful technique is known as a confusion matrix. Here we draw a grid
with each class along both the x and y axis. The x axis is the actual
number of items in each class and the y axis is the predicted number. In
a perfect classifier, there will be a diagonal line of values across the
grid moving from the top left to bottom right corresponding to the
number in each class, and all other cells will be zero. If any cell
outside of the diagonal is non-zero then it indicates a
miss-classification. Scikit-Learn has a function called
<code>confusion_matrix</code> in the <code>sklearn.metrics</code> class
which can display a confusion matrix for us. It will need two inputs:
arrays showing how many items were in each class for both the real data
and the classifications. We already have the real data in the
labels_test array, but we need to build it for the classifications by
classifying each image (in the same order as the real data) and storing
the result in another array.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># extract all test set predictions</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>y_test_pred <span class="op">=</span> mlp.predict(X_test)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>y_test_pred</span></code></pre>
</div>
<p>The <code>ConfusionMatrixDisplay</code> class in the
<code>sklearn.metrics</code> package can create a graphical
representation of a confusion matrix with colour coding to highlight how
many items are in each cell. This colour coding can be useful when
working with very large numbers of classes.</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>ConfusionMatrixDisplay.from_predictions(y_test,y_test_pred)</span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="cross-validation">Cross-validation<a class="anchor" aria-label="anchor" href="#cross-validation"></a>
</h2>
<hr class="half-width">
<p>Previously we split the data into training and test sets. But what if
the test set includes important features we want to train on that happen
to be missing in the training set? We are throwing away part of our data
to use it in the testing set.</p>
<p>Cross-validation runs the training/testing multiple times but splits
the data in a different way each time. This means all of the data gets
used both for training and testing. We can use multiple iterations of
training with different data in each set to eventually include the
entire dataset.</p>
<p>example list</p>
<p>[1,2,3,4,5,6,7,8]</p>
<p>train = 1,2,3,4,5,6 test = 7,8</p>
<p>train = 1,2,3,4,7,8 test = 5,6</p>
<p>train = 1,2,5,6,7,8 test = 3,4</p>
<p>train = 3,4,5,6,7,8 test = 1,2</p>
<p>(generate an image of this)</p>
<div class="section level3">
<h3 id="cross-validation-code-example">Cross-validation code example<a class="anchor" aria-label="anchor" href="#cross-validation-code-example"></a>
</h3>
<p>The <code>sklearn.model_selection</code> module provides support for
doing k-fold cross validation in Scikit-Learn. It can automatically
partition our data for cross validation.</p>
<p>Import this and call it <code>skl_msel</code></p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="im">import</span> sklearn.model_selection <span class="im">as</span> skl_msel</span></code></pre>
</div>
<p>Now we can choose how many ways we would like to split our data
(three or four are common choices).</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>kfold <span class="op">=</span> skl_msel.KFold(<span class="dv">4</span>)</span></code></pre>
</div>
<p>Now we can loop through our data and test on each combination. The
<code>kfold.split</code> function returns two variables and we will have
our for loop work through both of them. The train variable will contain
a list of which items (by index number) we are currently using to train
and the test one will contain the list of which items we are going to
test on.</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="cf">for</span> (train, test) <span class="kw">in</span> kfold.split(data):</span></code></pre>
</div>
<p>Now inside the loop, we can select the data with
<code>data_train = data.iloc[train]</code> and
<code>labels_train = labels.iloc[train]</code>. In some versions of
Python/Pandas/Scikit-Learn, you might be able to use
<code>data_train = data[train]</code> and
<code>labels_train = labels[train]</code>. This is a useful Python
shorthand which will use the list of indices from <code>train</code> to
select which items from <code>data</code> and <code>labels</code> we
use. We can repeat this process with the test set.</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>    data_train <span class="op">=</span> data.iloc[train]</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>    labels_train <span class="op">=</span> labels.iloc[train]</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>    data_test <span class="op">=</span> data.iloc[test]</span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>    labels_test <span class="op">=</span> labels.iloc[test]</span></code></pre>
</div>
<p>Finally, we need to train the classifier with the selected training
data and then score it against the test data. The scores for each set of
test data should be similar.</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>    mlp.fit(data_train,labels_train)</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Testing set score"</span>, mlp.score(data_test, labels_test))</span></code></pre>
</div>
<p>Once we have established that the cross validation was ok, we can go
ahead and train using the entire dataset by doing
<code>mlp.fit(data,labels)</code>.</p>
<p>Here is the entire example program:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a><span class="im">import</span> sklearn.datasets <span class="im">as</span> skl_data</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a><span class="im">import</span> sklearn.neural_network <span class="im">as</span> skl_nn</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a><span class="im">import</span> sklearn.model_selection <span class="im">as</span> skl_msel</span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a>data, labels <span class="op">=</span> skl_data.fetch_openml(<span class="st">'mnist_784'</span>, version<span class="op">=</span><span class="dv">1</span>, return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a>data <span class="op">=</span> data <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a>mlp <span class="op">=</span> skl_nn.MLPClassifier(hidden_layer_sizes<span class="op">=</span>(<span class="dv">50</span>,), max_iter<span class="op">=</span><span class="dv">50</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-10"><a href="#cb24-10" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" tabindex="-1"></a>kfold <span class="op">=</span> skl_msel.KFold(<span class="dv">4</span>)</span>
<span id="cb24-12"><a href="#cb24-12" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" tabindex="-1"></a><span class="cf">for</span> (train, test) <span class="kw">in</span> kfold.split(data):</span>
<span id="cb24-14"><a href="#cb24-14" tabindex="-1"></a>    data_train <span class="op">=</span> data.iloc[train]</span>
<span id="cb24-15"><a href="#cb24-15" tabindex="-1"></a>    labels_train <span class="op">=</span> labels.iloc[train]</span>
<span id="cb24-16"><a href="#cb24-16" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" tabindex="-1"></a>    data_test <span class="op">=</span> data.iloc[test]</span>
<span id="cb24-18"><a href="#cb24-18" tabindex="-1"></a>    labels_test <span class="op">=</span> labels.iloc[test]</span>
<span id="cb24-19"><a href="#cb24-19" tabindex="-1"></a>    mlp.fit(data_train,labels_train)</span>
<span id="cb24-20"><a href="#cb24-20" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Training set score"</span>, mlp.score(data_train, labels_train))</span>
<span id="cb24-21"><a href="#cb24-21" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Testing set score"</span>, mlp.score(data_test, labels_test))</span>
<span id="cb24-22"><a href="#cb24-22" tabindex="-1"></a>mlp.fit(data,labels)</span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="deep-learning">Deep learning<a class="anchor" aria-label="anchor" href="#deep-learning"></a>
</h2>
<hr class="half-width">
<p>Deep learning usually refers to newer neural network architectures
which use a special type of network known as a ‘convolutional network’.
Typically, these have many layers and thousands of neurons. They are
very good at tasks such as image recognition but take a long time to
train and run. They are often used with GPUs (Graphical Processing
Units) which are good at executing multiple operations simultaneously.
It is very common to use cloud computing or high performance computing
systems with multiple GPUs attached.</p>
<p>Scikit-Learn is not really setup for deep learning. We will have to
rely on other libraries. Common choices include Google’s TensorFlow,
Keras, (Py)Torch or Darknet. There is, however, an interface layer
between sklearn and tensorflow called skflow. A short example of using
this layer can be found at <a href="https://www.kdnuggets.com/2016/02/scikit-flow-easy-deep-learning-tensorflow-scikit-learn.html" class="external-link">https://www.kdnuggets.com/2016/02/scikit-flow-easy-deep-learning-tensorflow-scikit-learn.html</a>.</p>
<div class="section level3">
<h3 id="cloud-apis">Cloud APIs<a class="anchor" aria-label="anchor" href="#cloud-apis"></a>
</h3>
<p>Google, Microsoft, Amazon, and many other companys now have cloud
based Application Programming Interfaces (APIs) where you can upload an
image and have them return you the result. Most of these services rely
on a large pre-trained (and often proprietary) neural network.</p>
<div id="exercise-try-cloud-image-classificati" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="exercise-try-cloud-image-classificati" class="callout-inner">
<h3 class="callout-title">Exercise: Try cloud image classificati</h3>
<div class="callout-content">
<p>Take a photo with your phone camera or find an image online of a
common daily scene. Upload it to Google’s Vision AI at <a href="https://cloud.google.com/vision/" class="external-link uri">https://cloud.google.com/vision/</a> How many objects has it
correctly classified? How many did it incorrectly classify? Try the same
image with Microsoft’s Computer Vision API at <a href="https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/" class="external-link uri">https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/</a>
Does it do any better/worse than Google?</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Perceptrons are artificial neurons which build neural networks.</li>
<li>A perceptron takes multiple inputs, multiplies each by a weight
value and sums the weighted inputs. It then applies an activation
function to the sum.</li>
<li>A single perceptron can solve simple functions which are linearly
separable.</li>
<li>Multiple perceptrons can be combined to form a neural network which
can solve functions that aren’t linearly separable.</li>
<li>We can train a whole neural network with the back propagation
algorithm. Scikit-learn includes an implementation of this
algorithm.”</li>
<li>Training a neural network requires some training data to show the
network examples of what to learn</li>
<li>To validate our training we split the training data into a training
set and a test set.</li>
<li>To ensure the whole dataset can be used in training and testing we
can train multiple times with different subsets of the data acting as
training/testing data. This is called cross validation.</li>
<li>Deep learning neural networks are a very powerful modern machine
learning technique. Scikit-Learn does not support these but other
libraries like Tensorflow do.</li>
<li>Several companies now offer cloud APIs where we can train neural
networks on powerful computers.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-08-ethics"><p>Content from <a href="08-ethics.html">Ethics and the Implications of Machine Learning</a></p>
<hr>
<p>Last updated on 2025-07-23 |

        <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/edit/main/episodes/08-ethics.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the ethical implications of using machine learning in
research?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Consider the ethical implications of machine learning, in general,
and in research.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="ethics-and-machine-learning">Ethics and machine learning<a class="anchor" aria-label="anchor" href="#ethics-and-machine-learning"></a>
</h2>
<hr class="half-width">
<p>As machine learning has risen in visibility, so to have concerns
around the ethics of using the technology to make predictions and
decisions that will affect people in everyday life. For example:</p>
<ul>
<li>The first death from a driverless car which failed to brake for a
pedestrian.<a href="https://www.forbes.com/sites/meriameberboucha/2018/05/28/uber-self-driving-car-crash-what-really-happened/" class="external-link"><span class="math display">\[1\]</span></a>
</li>
<li>Highly targetted advertising based around social media and internet
usage. <a href="https://www.wired.com/story/big-tech-can-use-ai-to-extract-many-more-ad-dollars-from-our-clicks/" class="external-link"><span class="math display">\[2\]</span></a>
</li>
<li>The outcomes of elections and referenda being influenced by highly
targetted social media posts. This is compounded by data being obtained
without the user’s consent. <a href="https://www.vox.com/policy-and-politics/2018/3/23/17151916/facebook-cambridge-analytica-trump-diagram" class="external-link"><span class="math display">\[3\]</span></a>
</li>
<li>The widespread use of facial recognition technologies. <a href="https://www.bbc.co.uk/news/technology-44089161" class="external-link"><span class="math display">\[4\]</span></a>
</li>
<li>The potential for autonomous military robots to be deployed in
combat. <a href="https://www.theverge.com/2021/6/3/22462840/killer-robot-autonomous-drone-attack-libya-un-report-context" class="external-link"><span class="math display">\[5\]</span></a>
</li>
</ul></section><section><h2 class="section-heading" id="problems-with-bias">Problems with bias<a class="anchor" aria-label="anchor" href="#problems-with-bias"></a>
</h2>
<hr class="half-width">
<p>Machine learning systems are often argued to be be fairer and more
impartial in their decision-making than human beings, who are argued to
be more emotional and biased, for example, when sentencing criminals or
deciding if someone should be granted bail. But there are an increasing
number of examples where machine learning systems have been exposed as
biased due to the data they were trained on. This can occur due to the
training data being unrepresentative or just under representing certain
cases or groups. For example, if you were trying to automatically screen
job candidates and your training data consisted only of people who were
previously hired by the company, then any biases in employment processes
would be reflected in the results of the machine learning.</p>
</section><section><h2 class="section-heading" id="problems-with-explaining-decisions">Problems with explaining decisions<a class="anchor" aria-label="anchor" href="#problems-with-explaining-decisions"></a>
</h2>
<hr class="half-width">
<p>Many machine learning systems (e.g. neural networks) can’t really
explain their decisions. Although the input and output are known, trying
to explain why the training caused the network to behave in a certain
way can be very difficult. When decisions are questioned by a human it’s
difficult to provide any rationale for how a decision was arrived
at.</p>
</section><section><h2 class="section-heading" id="problems-with-accuracy">Problems with accuracy<a class="anchor" aria-label="anchor" href="#problems-with-accuracy"></a>
</h2>
<hr class="half-width">
<p>No machine learning system is ever 100% accurate. Getting into the
high 90s is usually considered good. But when we’re evaluating millions
of data items this can translate into 100s of thousands of
mis-identifications. This would be an unacceptable margin of error if
the results were going to have major implications for people, such as
criminal sentencing decisions or structuring debt repayments.</p>
</section><section><h2 class="section-heading" id="energy-use">Energy use<a class="anchor" aria-label="anchor" href="#energy-use"></a>
</h2>
<hr class="half-width">
<p>Many machine learning systems (especially deep learning) need vast
amounts of computational power which in turn can consume vast amounts of
energy. Depending on the source of that energy this might account for
significant amounts of fossil fuels being burned. It is not uncommon for
a modern GPU-accelerated computer to use several kilowatts of power.
Running this system for one hour could easily use as much energy a
typical home in the OECD would use in an entire day. Energy use can be
particularly high when models are constantly being retrained or when
“parameter sweeps” are done to find the best set of parameters to train
with.</p>
</section><section><h2 class="section-heading" id="ethics-of-machine-learning-in-research">Ethics of machine learning in research<a class="anchor" aria-label="anchor" href="#ethics-of-machine-learning-in-research"></a>
</h2>
<hr class="half-width">
<p>Not all research using machine learning will have major ethical
implications. Many research projects don’t directly affect the lives of
other people, but this isn’t always the case.</p>
<p>Some questions you might want to ask yourself (and which an ethics
committee might also ask you):</p>
<ul>
<li>Will the results of your machine learning influence a decision that
will have a significant effect on a person’s life?</li>
<li>Will the results of your machine learning influence a decision that
will have a significant effect on an animial’s life?</li>
<li>Will you be using any people to create your training data, and if
so, will they have to look at any disturbing or traumatic material
during the training process?</li>
<li>Are there any inherent biases in the dataset(s) you’re using for
training?</li>
<li>How much energy will this computation use? Are there more efficient
ways to get the same answer?</li>
</ul>
<div id="exercise-ethical-implications-of-your-own-research" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="exercise-ethical-implications-of-your-own-research" class="callout-inner">
<h3 class="callout-title">Exercise: Ethical implications of your own research</h3>
<div class="callout-content">
<p>Split into pairs or groups of three. Think of a use case for machine
learning in your research areas. What ethical implications (if any)
might there be from using machine learning in your research? Write down
your group’s answers in the etherpad.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>The results of machine learning reflect biases in the training and
input data.</li>
<li>Many machine learning algorithms can’t explain how they arrived at a
decision.</li>
<li>Machine learning can be used for unethical purposes.</li>
<li>Consider the implications of false positives and false
negatives.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-09-learn-more"><p>Content from <a href="09-learn-more.html">Find out more</a></p>
<hr>
<p>Last updated on 2025-07-23 |

        <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/edit/main/episodes/09-learn-more.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Where can you find out more about machine learning?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Know where to go to learn more about machine learning</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="other-algorithms">Other algorithms<a class="anchor" aria-label="anchor" href="#other-algorithms"></a>
</h2>
<hr class="half-width">
<p>There are many other machine learning algorithms that might be
suitable for helping you to answer your research questions.</p>
<p>The Scikit-Learn <a href="https://scikit-learn.org/stable/index.html" class="external-link">webpage</a> has a good
overview of all the features available in the library.</p>
</section><section><h2 class="section-heading" id="genetic-algorithms">Genetic algorithms<a class="anchor" aria-label="anchor" href="#genetic-algorithms"></a>
</h2>
<hr class="half-width">
<p>Genetic algorithms are techniques which try to mimic biological
evolution. They will learn to solve a problem through a gradual process
of simulated evolution. Each generation is mutated slightly and then
evaluated with a fitness function. The fittest “genes” will then be
selected for the next generation. Sometimes this is combined with neural
networks to change the networks size structure.</p>
<p>This <a href="https://www.youtube.com/watch?v=qv6UVOQ0F44" class="external-link">video</a>
shows a genetic algorithm evolving neural networks to play a video
game.</p>
</section><section><h2 class="section-heading" id="useful-resources">Useful Resources<a class="anchor" aria-label="anchor" href="#useful-resources"></a>
</h2>
<hr class="half-width">
<ul>
<li><p><a href="https://vas3k.com/blog/machine_learning/" class="external-link">Machine
Learning for Everyone</a> - A useful overview of many different machine
learning techniques, all introduced in an easy to follow way.</p></li>
<li><p><a href="https://developers.google.com/machine-learning/crash-course/" class="external-link">Google
machine learning crash course</a> - A quick course from Google on how to
use some of their machine learning products.</p></li>
<li><p><a href="https://research.fb.com/the-facebook-field-guide-to-machine-learning-video-series/" class="external-link">Facebook
Field Guide to Machine Learning</a> - A good introduction to machine
learning concepts from Facebook.</p></li>
<li><p><a href="https://docs.aws.amazon.com/machine-learning/latest/dg/amazon-machine-learning-key-concepts.html" class="external-link">Amazon
Machine Learning guide</a> - An introduction to the key concepts in
machine learning from Amazon.</p></li>
<li><p><a href="https://azure.microsoft.com/en-gb/overview/ai-platform/" class="external-link">Azure
AI</a> - Microsoft’s Cloud based AI platform.</p></li>
</ul>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>This course has only touched on a few areas of machine learning and
is designed to teach you just enough to do something useful.</li>
<li>Machine learning is a rapidly evolving field and new tools and
techniques are constantly appearing.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/" class="external-link">Source</a></p>
				<p><a href="https://github.com/jonathan-hartman/machine-learning-novice-sklearn/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:hartman@itc.rwth-aachen.de">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.12" class="external-link">sandpaper (0.16.12)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.6" class="external-link">varnish (1.0.6)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://jonathan-hartman.github.io/machine-learning-novice-sklearn/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "machine learning, python, sklearn, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://jonathan-hartman.github.io/machine-learning-novice-sklearn/aio.html",
  "identifier": "https://jonathan-hartman.github.io/machine-learning-novice-sklearn/aio.html",
  "dateCreated": "2025-07-23",
  "dateModified": "2025-07-23",
  "datePublished": "2025-07-23"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

